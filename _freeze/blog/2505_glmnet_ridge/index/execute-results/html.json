{
  "hash": "7ee2f0eba05b1b38b401355b59d596dc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"How does `glmnet` perform ridge regression?\"\nauthor: Thom Benjamin Volker\ntoc: true\nword-count: true\ndate: 2025-05-23\ncategories: [machine learning, regression]\nmonofont: \"Fira Code\"\n---\n\n# How I digress from my actual work\n\n\nToday, I was doing some programming for the `R` software `mice`, and it turns out I needed to calculate the residual degrees of freedom after performing regression with a ridge penalty to improve numerical stability. \nPerhaps, I thought, the `glmnet` package might provide any help, and at least may serve to verify my understanding. \nAlthough I love the `glmnet` package, I should've known better. \nThat is, the package is great for estimation, but makes some design choices that are buried somewhere deep in the documentation, which can make it hard to find what is actually going on under the hood. \nIn the end, I found out how `glmnet` performs ridge regression, so all good, I suppose.\n\n# Ridge regression\n\nRidge regression is a regression technique that is highly useful when there are relatively many predictors, or when predictors are highly correlated (i.e., when the $n \\times p$ predictor matrix $X$ is (close to) rank deficient).\nIn such instances, the usual least-squares solution $\\hat\\beta = (X'X)^{-1}X'y$ may have very large variance, and may be impossible to compute. \nIn such instances,  adding a positive penalty to the diagonal of the crossproduct of the predictor matrix ensures invertibility and reduces the variance of the resulting estimates.\nAn alternative way to think about this penalty is as a penalty on this size of the regression coefficients. \nThe larger this penalty, the closer the coefficients will be to zero. \nHence, ridge regression can be considered a compromise between the least-squares solution and a solution that shrinks all coefficients towards zero. \n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show figure code\"}\nlibrary(ggplot2)\n\nset.seed(123)\n\nX <- rnorm(200) |> matrix(100) %*% chol(-0.5 + 1.5 * diag(2))\ny <- X %*% c(1, 2) + rnorm(100)\n\nfit <- glmnet::glmnet(X, y, alpha = 0, nlambda = 200, lambda.min.ratio = 0.0000001)\ncoefs <- t(as.matrix(coef(fit)[-1,]))\n\nfit_ols <- lm(y~X)\ncoef_ols <- coef(fit_ols)\n\nx <- expand.grid(\n  beta1 = seq(-0.2, 1.5, length.out = 500),\n  beta2 = seq(-0.2, 2.5, length.out = 500)\n)\n\nx <- cbind(\n  x, z = mvtnorm::dmvnorm(x, mean = coef_ols[2:3], sigma = vcov(fit_ols)[2:3, 2:3])\n)\n\nx$z[x$z < 0.00001] <- NA\n\ncoefs <- coefs |>\n  as.matrix() |>\n  as.data.frame() |>\n  setNames(c(expression(beta[1]), expression(beta[2])))\n\nggplot(coefs, aes(x = `beta[1]`, y = `beta[2]`)) +\n  geom_point(col = \"#12244d\", size = 1) +\n  geom_path(col = \"#12244d\") +\n  labs(\n    x = parse(text = expression(beta[1])),\n    y = parse(text = expression(beta[2])),\n    title = \"Ridge regression solution path\"\n  ) +\n  stat_contour_filled(\n    data = x,\n    mapping = aes(x = beta1, y = beta2, z = z),\n    alpha = 0.5\n  ) +\n  theme_minimal() +\n  theme(plot.background = element_rect(fill = \"#fffbf2\", colour = \"transparent\")) +\n  xlim(-0.2, 1.5) +\n  ylim(-0.2, 2.5) +\n  geom_text(\n    aes(x = coef_ols[2] + 0.1, y = coef_ols[3]+0.1, label = as.character(expression(hat(beta)[ols]))), \n    parse = TRUE\n  )\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nThe ridge regression problem is commonly denoted as \n$$\n\\min_\\beta \\left\\{ ||y - \\beta_0 - X\\beta||_2^2 + \\lambda ||\\beta||_2^2 \\right\\},\n$$\nwhere $||\\cdot||_2$ denotes the $\\ell_2$ norm, $y$ is the response vector, $\\beta_0$ denotes an intercept, $\\beta$ denotes the regression coefficients and $\\lambda$ is a positive hyperparameter that controls the amount of shrinkage. \nFirst, we make explicit that the intercept term $\\beta_0$ is typically not penalized, because it simply denotes the conditional mean of $y$ if all predictors are zero. \nIt is not hard to see that when $\\lambda = 0$, the problem reduces to the least-squares problem, and when $\\lambda \\to \\infty$, the penalty dominates the fit of the model, and $\\beta \\to 0$. \n\nFor now, we ignore the intercept, and denote $\\tilde y = y - \\beta_0$. \nTo minimize the objective\n$$\n\\left\\{ ||\\tilde y - X\\beta||_2^2 + \\lambda ||\\beta||_2^2 \\right\\}\n$$\nwe expand it as\n$$\n\\min_\\beta \\left\\{ \\tilde y^T\\tilde y - 2\\tilde y^TX\\beta + \\beta^TX^TX\\beta + \\lambda\\beta^T\\beta \\right\\},\n$$\nusing the inner product $||a||_2^2 = a^Ta$, where $T$ denotes the transpose operator. To find the optimal parameters, we take the derivative and set it to zero, which yields\n$$\n\\begin{aligned}\n\\nabla_\\beta \\left\\{ \\tilde y^T\\tilde y - 2\\tilde y^TX\\beta + \\beta^TX^TX\\beta + \\lambda\\beta^T\\beta \\right\\} &= 0 \\\\\n\\implies -2X^T\\tilde y + 2X^TX\\beta + 2\\lambda\\beta &= 0 \\\\\n\\implies (X^TX + \\lambda I_p)\\beta &= X^T\\tilde y  \\\\\n\\implies (X^TX + \\lambda I_p)^{-1}X^T\\tilde y &= \\beta.\n\\end{aligned}\n$$\nwhere $I_p$ is the $p \\times p$ identity matrix. So far, nothing new under the sun. \n\n# Ridge regression with `glmnet`\n\nTo see how `glmnet` performs ridge regression, we simply use the `glmnet` function  with the `alpha = 0` argument.^[By default, `glmnet` fits a lasso regression model, which is fairly similar but uses an $\\ell_1$ penalty for the coefficients, which results in some coefficients estimated to be precisely zero. Changing the `alpha` parameter yields a mixture between `1-alpha` times the ridge penalty and `alpha` times the lasso penalty.]\nBelow, we generate a data set of $n = 200$ observations and $p=5$ predictors, using variance-covariance matrix\n$$\nV = D\\Sigma D,\n$$\nwhere $D = \\text{diag}(\\sqrt 1, \\sqrt 2, \\dots, \\sqrt 10)$ and $\\Sigma$ is a correlation matrix with $\\rho_{ij} = 0.5$ for $i \\neq j$.\nMoreover, all predictors have mean zero, and the response is generated as \n$$\ny = X\\beta + \\epsilon,\n$$\nwhere $\\beta = (0.01, 0.02, \\dots, 0.1)^T$ and $\\epsilon \\sim N(0, I_n\\sigma^2)$ with $\\sigma^2 = 1$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nN <- 200\nP <- 10\nV <- sqrt(diag(1:P)) %*% (0.5 + 0.5 * diag(P)) %*% sqrt(diag(1:P))\nX <- rnorm(N*P) |> matrix(N) %*% chol(V)\ncolnames(X) <- paste0(\"X\", 1:P)\ny <- X %*% ((1:P)/P^2) + rnorm(N)\n```\n:::\n\n\nFor the sake of exposition, we choose a single $\\lambda$ value and fix it to $1.5$, although we would normally use a more principled approach to select an optimal value for $\\lambda$ (e.g., cross-validation). Then, we can proceed the fit the model using `glmnet`. To keep it simple initially, we don't estimate an intercept, nor do we standardize the predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n\nlambda <- 1.5\n\nfit <- glmnet(\n  x = X, \n  y = y, \n  alpha = 0, \n  lambda = lambda, \n  standardize = FALSE, \n  intercept = FALSE\n)\n\nglmnet_coefs <- coef(fit)[,1]\nround(glmnet_coefs, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          X1          X2          X3          X4          X5 \n      0.000       0.005       0.025       0.026       0.026       0.081 \n         X6          X7          X8          X9         X10 \n      0.070       0.062       0.081       0.079       0.096 \n```\n\n\n:::\n:::\n\n\nThe question then is: how can we obtain the same estimates without using the `glmnet` function. The optimization problem is clear enough, so this should be doable. Let's try a first naive approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- lambda * diag(P)\nb <- solve(crossprod(X) + D, crossprod(X, y))\n\nround(b, 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      [,1]\nX1  -0.020\nX2   0.022\nX3   0.022\nX4   0.019\nX5   0.097\nX6   0.078\nX7   0.061\nX8   0.086\nX9   0.083\nX10  0.100\n```\n\n\n:::\n:::\n\n\nWell, clearly, the coefficients are not the same. Typically, they are somewhat larger than the `glmnet` estimates, although this doesn't hold for each coefficient. Well, okay, this was a bit stupid, because while my formulation above minimizes the penalized sum of squares, the `glmnet` function minimizes the penalized mean squared error. \nTo account for this adjustment, we can multiple our penalty parameter with the sample size. \nStill, if we would make this modification, solely adjusting the penalty term won't suffice, because `glmnet` also assumes $y$ to have unit variance (using the biased variance estimator). \nSince we don't estimate an intercept, we can suffice by multiplying the mean squared error by the standard deviation of $y$, or, equivalently, divide our penalty parameter by the standard deviation of $y$.\nHence, instead of minimizing\n$$\n\\left\\{ ||y - X\\beta||_2^2 + \\lambda ||\\beta||_2^2 \\right\\},\n$$\nwe should minimize\n$$\n\\left\\{||y - X\\beta||_2^2 + \\frac{N}{\\sigma_y} \\lambda ||\\beta||_2^2 \\right\\}.\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvary <- c(crossprod(y - mean(y)) / N)\nb <- solve(crossprod(X) + N/sqrt(vary) * D, \n           crossprod(X, y))\n```\n:::\n\n\nNow all coefficients are correct up to the same three decimals. Note that `glmnet` uses some numerical optimization scheme. Increasing the estimation precision yields almost identical coefficients. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefs_precise <- glmnet(\n  x = X, \n  y = y, \n  alpha = 0, \n  lambda = lambda, \n  standardize = FALSE, \n  intercept = FALSE,\n  thres = 1e-16,\n  maxit = 999999999\n) |> coef()\n\ncoefs_precise[2:11] - b\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]\nX1  -5.825564e-06\nX2  -1.186575e-06\nX3  -1.984316e-06\nX4  -3.108464e-06\nX5   7.272072e-06\nX6   3.519795e-06\nX7   5.148535e-08\nX8   3.079150e-06\nX9   2.021469e-06\nX10  2.165650e-06\n```\n\n\n:::\n:::\n\n\nSo, taking some shortcuts, we can obtain the same estimates as `glmnet`. Now, let's see if we can get the same estimates without taking any shortcuts (fortunately, we can, because otherwise I wouldn't have written this up).\n\nBefore proceeding, let's estimate the ridge regression coefficients with `glmnet` using all default settings.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndefault_fit <- glmnet(\n  x = X, \n  y = y, \n  alpha = 0, \n  lambda = lambda\n)\n\ncoef(default_fit)[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          X1          X2          X3          X4          X5 \n-0.10793832  0.05707674  0.06200104  0.05362275  0.04152741  0.06661149 \n         X6          X7          X8          X9         X10 \n 0.06013510  0.05024936  0.05856917  0.05292443  0.05773440 \n```\n\n\n:::\n:::\n\nBy now, we know that `glmnet` standardizes predictors and outcomes, so let's start from there.\nHowever, when estimating the coefficients on the standardized data, we need to backtransform the coefficients to the original scale. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nXscaled <- scale(X) * sqrt((N/(N-1)))\nyscaled <- scale(y) * sqrt((N/(N-1)))\n\ncorrection <- N / attr(yscaled, \"scaled:scale\") * sqrt(N/(N-1)) * D\nbacktransform <- attr(yscaled, \"scaled:scale\") / attr(Xscaled, \"scaled:scale\")\n\nb <- solve(crossprod(Xscaled) + correction, crossprod(Xscaled, yscaled)) * backtransform\nb\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          [,1]\nX1  0.05709667\nX2  0.06204284\nX3  0.05366507\nX4  0.04155725\nX5  0.06662820\nX6  0.06014292\nX7  0.05023970\nX8  0.05855390\nX9  0.05291095\nX10 0.05772391\n```\n\n\n:::\n:::\n\n\nHooray! The coefficients look very similar to the `glmnet` coefficients. \nBut wait.. We are missing the intercept still. \nFortunately, it is not too hard to add the intercept back in. Recall that the intercept is simply the expected value of $y$ when all predictors are equal to zero. \nBy centering each predictor, this corresponds to the expected value of $y$ when all predictors are equal to their mean.\nSo, we can obtain the intercept by taking the mean of $y$ and subtract the mean of each predictor multiplied with the respective regression coefficient.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- mean(y) - colMeans(X) %*% b\n```\n:::\n\n\nTo finish it off, we can estimate the `glmnet` model with larger precision again, and compare our estimates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- glmnet(\n  x = X, \n  y = y, \n  alpha = 0, \n  lambda = lambda, \n  standardize = TRUE, \n  intercept = TRUE,\n  maxit = 999999999,\n  thresh = 1e-16\n)\n\ncoef(fit)[,1] - c(b0, b)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  (Intercept)            X1            X2            X3            X4 \n-9.607908e-11 -1.282528e-09 -2.082874e-09 -1.951065e-09 -1.289601e-09 \n           X5            X6            X7            X8            X9 \n-6.446498e-10 -2.111742e-10  5.170960e-10  7.362262e-10  6.153778e-10 \n          X10 \n 4.423159e-10 \n```\n\n\n:::\n:::\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}