{
  "hash": "46d4b5fa9e49f0ee9fdd8edbed13b0b3",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Different ways of calculating OLS regression coefficients (in `R`)\"\nauthor: Thom Benjamin Volker\ndescription: \"Many different ways of calculating OLS regression coefficients exist, but some ways are more efficient than others. In this post we discuss some of the most common ways of calculating OLS regression coefficients, and how they relate to each other. Throughout, I assume some knowledge of linear algebra (i.e., the ability to multiply matrices), but other than that, I tried to simplify everything as much as possible.\"\ntoc: true\nword-count: true\ndate: 2025-06-16\ncategories: [regression, qr decomposition, singular value decomposition, gradient descent, machine learning]\neditor: \n  markdown: \n    wrap: sentence\n---\n\nLinear regression is life.\nAt age 17, Carl Friedrich Gauss, a German polymath, thought to himself: \"You know what, we need to be able to determine the best linear fit through this cloud of points\", and the rest is history.\nThe goal is to predict a target variable, $y_i$, for observation $i$ from a set of predictors, $x_{i,1}, x_{i,2}, \\ldots, x_{i,p}$.\nThe model is given by $$\ny_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\ldots + \\beta_p x_{i,p} + \\varepsilon_i,\n$$ where $\\beta_0$ denotes the intercept, $\\beta_1, \\ldots, \\beta_p$ are the regression coefficients corresponding to predictors $x_{i,1}, \\dots, x_{i,p}$, and $\\varepsilon_i$ is the error term.\nThe intercept $\\beta_0$ can be interpreted as the expected value of $y$ for an observation that scores zero on all predictor variables, and $\\beta_1, \\ldots, \\beta_p$ represent the change in the expected value of $y$ for a one-unit increase in the corresponding predictor variable, holding all other predictors constant.\nIt is often convenient to cast the model in matrix form: $$\ny = X \\beta + \\varepsilon,\n$$ where $y$ is the $n \\times 1$ response vector, $X$ is the $n \\times (p + 1)$ design matrix containing the predictors, $\\beta$ is the $(p + 1) \\times 1$ vector of coefficients, and $\\varepsilon$ is the $n \\times 1$ vector of errors.\nNote that the design matrix $X$ contains a column of ones for the intercept, which we denote throughout as the \"zeroth\" column so that it corresponds to $\\beta_0$, notation-wise.\nHence, for each observation, we multiply each predictor score by the corresponding regression coefficient and sum the terms.\n\n::: {.callout-note title=\"Intermezzo: Generating some example data\"}\nBefore we continue, we generate some example data that we will use throughout.\nOur example data consists of two normally distributed predictor variables with means $\\mu_{X_1} = \\mu_{X_2} = 0$, variances $\\sigma^2_{X_1} = \\sigma^2_{X_2} = 1$ and correlation $\\rho_{X_1, X_2} = 0.5$.\nMoreover, the outcome variable is also distributed normally, with mean $\\mu_Y = 0.2 \\cdot X_1 + 0.5 \\cdot X_2$ and residual variance $\\sigma^2_\\varepsilon = 1$.\nIn matrix form, we have the following model: $$\nX \n\\sim \n\\mathcal{N} \n\\begin{pmatrix}\n\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \n\\begin{bmatrix}1 & .5 \\\\ .5 & 1 \\end{bmatrix}\n\\end{pmatrix}, ~~~\ny \\sim \\mathcal{N}(X \\beta, 1), ~~~\n\\beta = \\begin{bmatrix} 0.2 \\\\ 0.5 \\end{bmatrix}.\n$$ From this model, we generate $N = 200$ observations.\nIn `R`, this can be done as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\nN <- 200                            # sample size\nP <- 2                              # number of predictors\nV <- 0.5 + 0.5 * diag(P)            # covariance matrix of predictors\nX <- mvtnorm::rmvnorm(N, sigma = V) # generate predictors\ny <- X %*% c(0.2, 0.5) + rnorm(N)   # generate response variable\n```\n:::\n\n\nThroughout, we use an intercept in the estimation models, which we append to our `X` matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXd <- cbind(1, X)\n```\n:::\n\n:::\n\nThe goal, of course, is to estimate the regression coefficients $\\beta$ such that we can predict the response variable $y$ as good as it gets.\nYou might want to do this by minimising the sum of squared errors: $$\n\\min_\\beta \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_{i,1} - \\ldots - \\beta_p x_{i,p})^2 = (y - X \\beta)^T (y - X \\beta).\n$$ The \"best\" estimate for this problem yields the smallest residuals (in a squared error sense).\nSo, how do we choose the coefficients such that the residuals are as small as possible?\nOne way would be to start with an initial guess for the regression coefficients and iteratively move in the direction where the errors decrease as fast as possible.\nIf you look at the contour plot below, the residual sum of squares for some data $X$ and $y$ is displayed as a function of the two regression coefficients.\nAt every location in the contour plot, the sum of squared errors slopes down in the direction of the minimum.\nSo, if we knew how the sum of squares changes as a function of the regression coefficients, we could iteratively move our estimates of the regression coefficients in the direction where the residual sum of squares becomes smaller and smaller, until we end up at the minimum!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show figure code\"}\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbgrid <- expand.grid(\n  b1 = seq(-0.3, 0.7, length.out = 100),\n  b2 = seq(0, 1, length.out = 100)\n)\n\nopt <- coef(lm(y~Xd-1))\n\nrss <- function(b, x, y, b0) {\n  yhat <- b0 + X %*% b\n  sum((y - yhat)^2)\n}\n\nrss <- mutate(bgrid, rss = apply(bgrid, 1, rss, x = X, y = y, b0 = opt[1]))\n\nggplot(rss, aes(b1, b2, z = rss)) +\n  geom_contour_filled() +\n  geom_point(aes(x = opt[2], y = opt[3]), color = \"red\", size = 3) +\n  labs(\n    title = \"Residual sum of squares\",\n    subtitle = \"Contours of residual sum of squares as a \nfunction of the regression coefficients\",\n    x = \"Coefficient for predictor 1\",\n    y = \"Coefficient for predictor 2\"\n  ) +\n  theme_minimal() +\n  theme(plot.background = element_rect(fill = \"#fffbf2\", colour = \"transparent\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/fig-rss_contour-1.png){#fig-rss_contour width=672}\n:::\n:::\n\n\nTo know how the residual sum of squares changes as a function of the regression coefficient, we can make use of the derivatives of this function with respect to $\\beta$.\nLoosely speaking, the derivatives quantify how fast the sum of squares changes when we change each of the coefficients by a tiny amount.\nThe collection of these rates of change, a vector called the *gradient*, tells us in what direction we need to move to get closer to the minimum.\nThe gradient is commonly denoted by $\\nabla_\\beta$ (the $\\nabla$ symbol is called *nabla* or *del*) and can be obtained by applying the chain rule for differentiation: $$\n\\begin{aligned}\n\\nabla_\\beta &= \\begin{bmatrix} \n\\frac{\\partial}{\\partial \\beta_0} \n\\left(\\sum_{i=1}^n (y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots - \\beta_px_{i,p})^2 \\right)\\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_p} \\left(\\sum_{i=1}^n (y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots - \\beta_px_{i,p})^2 \\right)\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\left( \\sum_{i=1}^n -2 x_{i,0}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\vdots \\\\\n\\left( \\sum_{i=1}^n -2 x_{i,p}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\end{bmatrix}.\n\\end{aligned}\n$$ Each partial derivative tells us how the total error would change if we nudged one coefficient while keeping the others fixed.\nWe can update each regression coefficient repeatedly by a tiny amount in the direction of the minimum, until we ultimately reach this minimum.\nThis process is known as *gradient descent*.\nA short and applied illustration of gradient descent is included below, but since I don't want to focus this post on gradient descent too much, I hid the algorithm behind an expandable block.\n\n::: {.callout-tip collapse=\"true\" title = \"Click here to learn more about gradient descent!\"}\n\nGradient descent attempts to find the optimal values of the regression coefficients iteratively.\nTo do so, it starts at a given point, say, $\\beta_0^{(0)} = \\beta_1^{(0)} = \\beta_2^{(0)} = 0$.\nFrom here on, we can take a small step in the opposite direction of the gradient, as the gradient gives the direction of the fastest increase, while we want to decrease function values (i.e., the sum of squares) until we reach the minimum.\nSo, in the first iteration, we move $\\alpha$ steps in the direction of the gradient, towards the optimal $\\beta_0$ value given the current values for $\\beta_1$ and $\\beta_2$, so that we have $$\n\\beta_0^{(1)} \\leftarrow \\beta_0^{(0)} + \\alpha \n\\left(\n\\sum_{i=1}^n 2x_{i,0} (y_i - \\beta_0^{(0)}x_{i,0} - \\beta_1^{(0)} x_{i,1} - \\beta_2^{(0)} x_{i,2})\n\\right).\n$$ Note that the \"minus\" from the derivative cancels with the \"minus\" from moving in the opposite direction.\nSubsequently, we do the same for the other regression coefficients, such that $$\n\\begin{aligned}\n\\beta_1^{(1)} &\\leftarrow \\beta_1^{(0)} + \\alpha \n\\left(\n\\sum_{i=1}^n 2x_{i,1} (y_i - \\beta_0^{(0)}x_{i,0} - \\beta_1^{(0)} x_{i,1} - \\beta_2^{(0)} x_{i,2})\n\\right), \\\\\n\\beta_2^{(1)} & \\leftarrow \\beta_2^{(0)} + \\alpha \n\\left(\n\\sum_{i=1}^n 2x_{i,2} (y_i - \\beta_0^{(0)}x_{i,0} - \\beta_1^{(0)} x_{i,1} - \\beta_2^{(0)} x_{i,2})\n\\right),\n\\end{aligned}\n$$ For all subsequent steps, we then have $$\n\\beta_j^{(k+1)} \\leftarrow \\beta_j^{(k)} + \\alpha \\left(\n\\sum_{i=1}^n 2x_{i,j} (y_i - \\beta_0^{(k)}x_{i,0} - \\beta_1^{(k)} x_{i,1} - \\beta_2^{(k)} x_{i,2}).\n\\right),\n$$ We can repeat this procedure until the regression coefficients stop changing, at which point the algorithm is said to have converged.\nA very naive implementation of this algorithm is implemented below.\nNote that many computations can be performed in more efficient ways.\nNowadays, many resources on gradient descent exist, most of which I did not find particularly instructive, but the [section on Gradient Descent in Dive Into Deep Learning](https://d2l.ai/chapter_optimization/gd.html) combines the mathematics with simple examples and nice figures, and the paper by [Sebastian Ruder (2017)](https://arxiv.org/pdf/1609.04747) covers many extensions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nderiv_beta_j <- function(X, y, beta, j) {\n  -2 * sum(X[,j] * (y - X %*% beta))\n}\n\nalpha <- 0.001                       # set the set-size\ntol   <- 1e-8                        # tolerance for convergence\niter  <- 1                           # initialize counter\nmaxit <- 1000                        # set maximum number of iterations\nconv  <- FALSE                       # state that we currently haven't converged\nbeta_matrix <- matrix(0, maxit, P+1) # initialize empty matrix for coefficients\n\nwhile(!conv) {\n  iter <- iter + 1\n  for (j in 1:ncol(Xd)) {\n    beta_matrix[iter, j] <- beta_matrix[iter-1, j] - \n      alpha * deriv_beta_j(Xd, y, beta_matrix[iter-1, ], j)\n  }\n  if (sum(abs(beta_matrix[iter, ] - beta_matrix[iter - 1, ])) < tol) {\n    conv <- TRUE\n  } \n  if (iter == maxit) {\n    cat(\"The algorithm didn't converge\")\n    break\n  }\n}\n```\n:::\n\n\nThe following figure shows how the algorithm moves from starting values to the optimum, and we can confirm that the solution is almost identical to the solution provided by simply running `lm()` in `R`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Show figure code\"}\nbeta_matrix <- beta_matrix[1:iter, ]\n\nggplot(rss, aes(b1, b2, z = rss)) +\n  geom_contour_filled() +\n  geom_point(data = NULL, aes(x = c(beta_matrix[,2], rep(NA, 10000 - nrow(beta_matrix))), \n                              y = c(beta_matrix[,3], rep(NA, 10000 - nrow(beta_matrix)))),\n             col = \"yellow\") +\n  geom_point(aes(x = opt[2], y = opt[3]), color = \"red\", size = 3) +\n  labs(\n    title = \"Residual sum of squares\",\n    subtitle = \"Contour plot of the residual sum of squares as a function of the regression coefficients\",\n    x = \"Coefficient for predictor 1\",\n    y = \"Coefficient for predictor 2\"\n  ) +\n  theme_minimal() +\n  theme(plot.background = element_rect(fill = \"#fffbf2\", colour = \"transparent\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/gradient-descent-figure-1.png){width=672}\n:::\n:::\n\n\nThis remarkably simple algorithm performs quite well in many situations, and can, with some modifications, even be the most efficient way to calculate linear regression coefficients (when the dataset is huge, in terms of sample size and number of variables).\n\n:::\n\nWe can also use the gradient directly, by noting that in the optimum, the derivatives are zero, because moving further in the direction of the minimum is not possible, and thus there is \"zero\" change in this direction.\nSetting each derivative equal to zero and solving the equations for the regression coefficients yields the optimal solution in one go.\nThis is something that can be done with *linear algebra*!\n\n## From derivatives to estimates\n\nIn the previous section, we saw that the gradient is given by $$\n\\nabla_\\beta = \\begin{bmatrix}\n\\left( \\sum_{i=1}^n -2 x_{i,0}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\vdots \\\\\n\\left( \\sum_{i=1}^n -2 x_{i,p}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\end{bmatrix}.\n$$ In each row of this vector, the product term consistently multiplies the columns of our predictor matrix, $X$, with the residuals $y - X\\beta$.\nThe same operations can be encoded in terms of matrix, $$\n\\nabla_\\beta = -2 X^T(y - X\\beta),\n$$ which multiplies each column of $X$ with the vector of residuals and sums the elements.\nWe can get rid of the parentheses, such that we obtain $$\n\\begin{aligned}\n\\nabla_\\beta &= -2 X^T(y - X\\beta) \\\\\n&= -2X^Ty + 2X^TX\\beta\n\\end{aligned}\n$$ Setting the derivative equal to zero and dividing both terms by $2$ yields the set of equations $$\nX^T X \\beta = X^T y,\n$$ which are commonly called the *normal equations*.\nWe can solve these equations for $\\beta$ by pre-multiplying both sides with the inverse of $X^T X$, and noting that $(X^T X)^{-1}(X^T X) = I$, the identity matrix.\nWe then obtain $$\n\\begin{aligned}\n(X^T X)^{-1}(X^T X)\\beta &= (X^T X)^{-1} X^T y \\\\\n\\beta &= (X^T X)^{-1} X^T y.\n\\end{aligned}\n$$ Hooray!\nWith just a few steps, we get from the gradient to the solution of the regression problem.\n\n## Solving the normal equations in `R`\n\nCoding the solution to our regression problem takes just a couple of operations.\nRecall that our design matrix `Xd` includes a column of ones for the intercept.\nThe solution is given by\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve(t(Xd) %*% Xd) %*% t(Xd) %*% y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n```\n\n\n:::\n:::\n\n\nWe can verify whether our obtained solution equals the solution provided by the regression function in `R` by calling `lm()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_fit <- lm(y ~ X)\ncoef(lm_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)          X1          X2 \n-0.05924251  0.12069667  0.52018686 \n```\n\n\n:::\n:::\n\n\nNot very surprisingly, the coefficients are equal.\nHowever, I can directly disclose that our solution is correct, but can be improved in various ways.\nFirst, inverting a matrix is expensive, and calculating the cross products as `t(Xd) %*% Xd` and `t(Xd) %*% y` is also not very efficient.\nWe can already achieve a speed-up by using the cross-product function `crossprod()`, which is typically more efficient than separately transposing and multiplying matrices.\nMoreover, we do not need to invert the matrix $X^T X$, but we can solve the normal equations directly, which also yields a speed-up (@tbl-benchmark provides some evidence for these claims).\nIn terms of code, this is achieved as follows, which again yields the same coefficients.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve(crossprod(Xd), crossprod(Xd, y))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n```\n\n\n:::\n:::\n\n\nThis is almost as efficient as it gets, computationally, because internally, `R` uses some clever tricks to solve this system of equations as efficiently as possible.\nHowever, there are other ways to obtaining the regression coefficients that have different advantages.\nIn the next sections, we look into some of the most popular linear algebra tricks to compute the regression coefficients.\n\n## Solving the normal equations through a QR decomposition\n\nAn alternative way to solve the normal equations is by first performing a transformation of the design matrix $X$ that simplifies the subsequent calculations.\nTo this end, we can decompose the design matrix $X$ into an $n \\times p+1$ orthogonal unitary matrix $Q$ and an upper triangular matrix $R$ using a QR decomposition.\nThis sounds really complicated, but it merely means that the matrix $Q$ contains $p+1$ uncorrelated (i.e., orthogonal) columns of length one (i.e., the sum of the squared elements of every column in $Q$ equals $\\sum_{i=1}^n Q_{(i,j)}^2 = ||Q_{.,j}||_2 = Q_{.,j}^TQ_{.,j} = 1$).\nThe matrix $R$ is then chosen such that when multiplied with $Q$, we obtain $X$ again.\nFor those unfamiliar with QR decompositions, a simple algorithm for computing the QR decomposition is included below.\n\n::: {.callout-tip collapse=\"true\"}\n## A simple algorithm for computing the QR decomposition (Gram-Schmidt)\n\nNote that the algorithm below is included solely for explanatory purposes for those unaware of the QR decomposition.\nIt is poor code from a computational point of view.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqr_func <- function(X) {\n  # store dimensions of input matrix\n  dim <- dim(X)\n  # initialize empty Q matrix\n  Q <- matrix(0, dim[1], dim[2])\n  # note the convenient feature of Q that the zero columns don't do anything\n  # initialize the first column of Q as a scaled version of the first column of X\n  Q[,1] <- X[,1] / c(sqrt(crossprod(X[,1])))\n  for (j in 2:dim[2]) {\n    # calculate coefficients that produce X[,j] orthogonal to existing Q columns\n    b <- c(crossprod(X[,j], Q[,1:(j-1)]) / apply(Q[,1:(j-1), drop = FALSE], 2, crossprod))\n    # note that these coefficients are merely the unscaled covariances of X[,j] \n    # with the existing columns of Q, and by multiplying existing Q with these \n    # coefficients and subtracting these values from X, the new Q-column is \n    # uncorrelated to the existing Q-columns. \n    Q[,j] <- X[,j] - Q[,1:(j-1), drop = FALSE] %*% b\n    # scale to unit length\n    Q[,j] <- Q[,j] / sqrt(sum(Q[,j]^2))\n  }\n  # calculate R by making sure that Q^T X = R, and thus X = QR\n  # we use here that Q is orthonormal, and thus Q^TQ = I\n  R <- t(Q) %*% X\n\n  list(Q = Q, R = R)\n}\n\nK <- matrix(sample(20), 5)\n\n(qr_own <- qr_func(K))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$Q\n           [,1]       [,2]        [,3]       [,4]\n[1,] 0.06696495  0.4354260 -0.11828785  0.3418044\n[2,] 0.20089486  0.7514209 -0.10300605  0.2878141\n[3,] 0.66964953  0.1928315  0.01205777 -0.7037544\n[4,] 0.53571962 -0.2341348  0.67174297  0.4547543\n[5,] 0.46875467 -0.3921322 -0.72388793  0.3134662\n\n$R\n              [,1]          [,2]         [,3]      [,4]\n[1,]  2.986637e+01  1.720999e+01 2.604937e+01 20.960030\n[2,]  1.776357e-15  1.802266e+01 7.528889e+00  8.005370\n[3,] -7.105427e-15 -5.995204e-15 8.645595e+00  1.010103\n[4,] -4.440892e-15 -9.547918e-15 2.442491e-15  6.524636\n```\n\n\n:::\n:::\n\n\nNote that the matrix $R$ is unique only up to multiplication by a diagonal matrix consisting of positive and negative ones.\nThat is, if we multiply a column of $Q$ by $-1$ and perform the same operation to the row of $R$, we obtain the same $X$ upon multiplication.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqr_R <- qr(K)\nQ <- qr.Q(qr_R)\nR <- qr.R(qr_R)\nsign_R <- sign(diag(R))\n\nall.equal(Q, qr_own$Q %*% diag(sign_R))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n\n```{.r .cell-code}\nall.equal(R, diag(sign_R) %*% qr_own$R)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n:::\n\nUsing some convenient features of orthonormal and upper triangular matrices, it is actually quite easy to perform linear regression right now.\nUsing the fact that $X = QR$, we can rewrite our linear regression problem as $$\n\\begin{aligned}\nX^TX \\beta &= X^Ty \\\\\n(QR)^T(QR) \\beta &= (QR)^T y \\\\\nR^TQ^TQR \\beta &= R^TQ^T y. \\\\\n\\end{aligned}\n$$ So far, it seems like we haven't achieved much, other than rewriting some expression into a longer and not necessarily simpler expression.\nHowever, remember that $Q$ is orthonormal, and thus that $Q^TQ = I$.\nSo, now we have $$\nR^TR \\beta = R^TQ^Ty,\n$$ which is almost as simple as the expression we already had.\nWe can again pre-multiply both sides with the inverse of $R^T$, which results in two new identity matrices that we can ignore.\nWe now obtain the final expression $$\nR\\beta = Q^T y,\n$$ and since $R$ is upper triangular, we can very efficiently solve this set of equations using back-substitution.\nIn our case, we have only two predictors and an intercept, so $R$ is $3 \\times 3$ with zeros below the diagonal, $\\beta$ is a vector with three elements, and $Q^Ty$ is a vector with three elements.\nHence, we have $$\n\\begin{aligned}\nR_{1,1}\\beta_1 + R_{1,2}\\beta_2 + R_{1,3}\\beta_3 &= (Q^T y)_1 \\\\\nR_{2,2}\\beta_2 + R_{2,3}\\beta_3 &= (Q^T y)_2 \\\\\nR_{3,3}\\beta_3 &= (Q^Ty)_3.\n\\end{aligned}\n$$ Starting from the bottom, $\\beta_3$ is given by $(Q^Ty)_3 / R_{3,3}$.\nThe value for $\\beta_2$ is easily obtained once we have $\\beta_3$!\nThe only thing we have to do is rewriting the equation above as $\\beta_2 = ((Q^T y)_{2} - R_{2,3} \\beta_3)/R_{2,2}$.\nDoing the same thing for the intercept yields $\\beta_1 = ((Q^T y)_{1} - R_{1,2}\\beta_2 -  R_{1,3} \\beta_3)/R_{1,1}$.\nYoo, this stuff is actually simpler than I thought!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nQR <- qr(Xd)\nQTy <- t(qr.Q(QR)) %*% y\nR <- qr.R(QR)\n\nb3 <- QTy[3] / R[3,3]\nb2 <- (QTy[2] - R[2,3] * b3) / R[2,2]\nb1 <- (QTy[1] - R[1,2] * b2 - R[1,3] * b3) / R[1,1]\n\nc(b1, b2, b3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.05924251  0.12069667  0.52018686\n```\n\n\n:::\n:::\n\n\n::: callout-tip\n## We can do the same thing much more efficient, just watch!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqr.solve(Xd, y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n```\n\n\n:::\n:::\n\n\nInternally, `solve(crossprod(X), crossprod(X, y))` uses a similar trick (by decomposing $X^TX = LU$), so we don't necessarily expect a speed-up here.\n:::\n\nUsing a $QR$ decomposition to solve the least-squares equations is computationally more stable (i.e., results in smaller approximation errors) than solving the normal equations directly.\nIf $X^TX$ is far from singular, this does not matter much, but if some variables are highly correlated, the resulting approximation errors can be quite large.\nHowever, the computation is somewhat less efficient than solving the normal equations directly.\nWe will come back to computational efficiency below.\n\n## Same trick, different (singular value) decomposition\n\nIf you get the gist of the previous method, the singular value decomposition will feel fairly easy.\nWe again start from the setting that $X^TX\\beta = X^Ty$, but now we replace $X$ by its singular value decomposition $U\\Sigma V^T$, where both $U$ and $V$ are orthonormal matrices and $\\Sigma$ is a diagonal matrix containing the singular values of $X$.\nWe now obtain the following expression $$\nV\\Sigma^T U^T U \\Sigma V^T \\beta = V \\Sigma^T U^T y.\n$$ We know from the previous section that the product of orthonormal matrices $U^T U$ can be ignored, and since $\\Sigma$ is diagonal, $\\Sigma^T = \\Sigma$.\nSo we can rewrite the expression as $$\n\\begin{aligned}\nV\\Sigma^T U^T U \\Sigma V^T \\beta &= V \\Sigma^T U^T y \\\\\n\\Rightarrow V\\Sigma\\Sigma V^T \\beta &= V \\Sigma U^T y \\\\\n\\Rightarrow \\Sigma\\Sigma V^T \\beta &= \\Sigma U^T y\\\\\n\\Rightarrow V^T \\beta &= \\Sigma^{-1} U^Ty \\\\\n\\Rightarrow \\beta &= V \\Sigma^{-1} U^T y,\n\\end{aligned}\n$$ where we performed the following operations:\n\n-   On line 2: remove $U^TU$, as it's just cluttered notation for an identity matrix.\n-   On line 3: premultiply both sides by $V^T$ and note that $V^TV$ is also an identity matrix.\n-   On line 4: premultiply both sides by $\\Sigma^{-1}\\Sigma^{-1}$.\n-   On line 5: premultiply both sides by $V$ (and use orthogonality of $V$).\n\nIn `R`, we thus have the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSV <- svd(Xd)\nUSV$v %*% diag(1/USV$d) %*% t(USV$u) %*% y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n```\n\n\n:::\n:::\n\n\nThese calculations can be further simplified because we do not need to construct the diagonal matrix $\\Sigma^{-1}$.\nWe can carry out these calculations element-wise on the $U^Ty$ vector, like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nUSV$v %*% (crossprod(USV$u, y)/USV$d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n```\n\n\n:::\n:::\n\n\nThe singular value decomposition is again slightly more expensive to compute than the $QR$ decomposition, and is also more stable computationally than solving the normal equations directly.\nMoreover, the singular value decomposition can be used even if $X^TX$ is rank-deficient, i.e., if some columns of $X$ are linearly dependent.\nIn such instances, we use $$\n\\Sigma^+ = \\begin{cases}\n\\frac{1}{\\sigma_i} & \\text{if } \\sigma_i>0 \\\\\n0 & \\text{otherwise}\n\\end{cases},\n$$ which is the pseudo-inverse of the matrix $\\Sigma$ that can be used to calculate the pseudo-inverse of a rank-deficient matrix $X^TX$.\nThis formulation can even be used if we have more variables than cases, which is sometimes called *minimum* $\\ell_2$ norm regression (see, for example, [Tibshirani, 2024](https://www.stat.berkeley.edu/~ryantibs/statlearn-s24/lectures/ridgeless.pdf) and [Kobak et al., 2020](https://www.jmlr.org/papers/volume21/19-844/19-844.pdf)).\n\n## One final decomposition (Cholesky)\n\nWe circle back to our initial solution $$\nX^TX\\beta = X^Ty,\n$$ and introduce a final decomposition to solve the set of equations.\nBy noting that $X^TX$ is symmetric and positive-definite, we can decompose it as $$\nX^TX = LL^T,\n$$ where $L$ is a lower-triangular matrix.\nSo, instead of decomposing the design matrix (i.e., the matrix with observed predictors), we decompose the (unscaled) covariance matrix $X^TX$.\nUsing this formulation, we can write $$\nLL^T\\beta = X^Ty.\n$$ \nNow, because $L$ is lower-triangular, and $L^T$ is upper-triangular, we can solve the set of equations efficiently as follows.\nFirst, we can solve \n$$\nL g = X^Ty\n$$ \nfor a newly introduced vector $g$, which is easy because $L$ is lower-triangular.\nTo do this, we use forward substitution, which implies nothing more than the following.\nStarting from the first row, we have $L_{11}g_1 = (X^Ty)_1$.\nFor the second row, we have $L_{21} g_1 + L_{22}g_2 = (X^Ty)_2$ for which we can plug in the previously calculated $g_1$. \nWe repeat the process until we solved for all elements of $g$.\nSubsequently, we can solve $L^T\\beta = g$, which is also easy because we can make use of the fact that $L^T$ is upper triangular, which means we can use backward substitution.\nHere, starting from the last row, we have $(L^T)_{pp}\\beta_p = g_p$, $(L^T)_{p-1}\\beta_{p-1} = g_{p-1}$, and so on.\nTo see why this works, we only have to rewrite the equation $$\n\\begin{aligned}\nLL^T\\beta &= X^Ty \\\\\n\\Rightarrow ~~~~ LL^T\\beta &= Lg ~~~~~~ \\text{rewrite } X^Ty \\text{ as } Lg \\\\\n\\Rightarrow ~~~~~~~ L^T\\beta &= g ~~~~~~~~ \\text{premultiply with } L^{-1}.\n\\end{aligned}\n$$ This is one of the most efficient ways to calculate the regression coefficients, but can be numerically unstable when $X$ is close to rank-deficient.\n\n::: {.callout-tip collapse=\"true\"}\n## Computing the Cholesky decomposition\n\nTo compute the Cholesky decomposition, we need to find a lower-triangular matrix $L$ such that, when multiplied with its transpose, it produces the original matrix $X^TX$.\nLet's break this down into a set of equations, where we write $X^TX = A = LL^T$: $$\n\\begin{aligned}\nA &= LL^T \\\\\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1p} \\\\\na_{21} & a_{22} & \\cdots & a_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{p1} & a_{p2} & \\cdots & a_{pp}\n\\end{bmatrix} &= \\begin{bmatrix} \nl_{11} & 0 & \\cdots & 0 \\\\\nl_{21} & l_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{p1} & l_{p2} & \\cdots & l_{pp}\n\\end{bmatrix} \\begin{bmatrix}\nl_{11} & l_{21} & \\cdots & l_{p1} \\\\\n0 & l_{22} & \\cdots & l_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & l_{pp}\n\\end{bmatrix}.\n\\end{aligned}\n$$ From here, it follows that, for the first row of $A$, we have $$\n\\begin{aligned}\na_{11} &= l_{11} \\cdot l_{11} + 0 \\cdot 0 + \\cdots + 0 \\cdot 0 = l_{11}^2 \\\\\na_{12} &= l_{11} \\cdot l_{21} + 0 \\cdot l_{22} + \\cdots + 0 \\cdot 0 = l_{11} \\cdot l_{21} \\\\\n\\vdots ~~ &= ~~~~~~~~~~~~~~~~~~~~~~\\vdots \\\\\na_{1p} &= l_{11} \\cdot l_{p1} + 0 \\cdot l_{p2} + \\cdots + 0 \\cdot l_{pp} = l_{11} \\cdot l_{p1}.\n\\end{aligned}\n$$ Thus, $l_{11} = \\sqrt{a_{11}}$, $l_{21} = a_{12}/l_{11}$, using $l_{11}$ obtained in the previous step, and $l_{p_1}=a_{1p}/l_{11}$, again using the previously obtained $l_{11}$.\nMoving to the second row of $A$, we have that $$\na_{21} = a_{12},\n$$ which we have obtained already.\nContinuing our journey, we get $$\n\\begin{aligned}\na_{22} &= l_{21} \\cdot l_{21} + l_{22} \\cdot l_{22} + 0 + \\cdots + 0 = l_{21}^2 + l_{22}^2 \\\\\n\\vdots ~~ &= ~~~~~~~~~~~~~~~~~~~~~~\\vdots \\\\\na_{2p} &= l_{21} \\cdot l_{p1} + l_{22} \\cdot l_{p2} + 0 + \\cdots + 0 = l_{21} \\cdot l_{p1} + l_{22} \\cdot l_{p2},\n\\end{aligned}\n$$ which we can again solve sequentially, $$\n\\begin{aligned}\nl_{22} &= \\sqrt{a_{22} - l_{21}} \\\\\n\\vdots &= ~~~~~~~~~~ \\vdots \\\\\nl_{p2} &= \\left( a_{2p} - l_{21} \\cdot l_{p1} \\right)/l_{22},\n\\end{aligned}\n$$ and noting that every required factor of $L$ has been calculated previously.\n\nPutting everything in a small algorithm, we obtain the following Cholesky-Crout algorithm (see [Wikipedia](https://en.wikipedia.org/wiki/Cholesky_decomposition#The_Cholesky%E2%80%93Banachiewicz_and_Cholesky%E2%80%93Crout_algorithms):\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncholesky <- function(XTX) {\n  dim <- dim(XTX)\n  L <- matrix(0, dim[1], dim[2])\n  L[1,1] <- sqrt(XTX[1,1])\n  for (i in 2:dim[1]) {\n    for (j in 1:(i-1)) {\n      L[i,j] <- (XTX[i,j] - sum(L[i,1:j]*L[j,1:j]))/L[j,j] # \n    }\n    L[i,i] <- sqrt(XTX[i,i] - sum(L[i,1:(i-1)]^2))\n  }\n  L\n}\n\nXTX <- crossprod(Xd)\nL <- cholesky(XTX)\n\nXTX\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]       [,2]        [,3]\n[1,] 200.0000000  -3.838651  -0.1295953\n[2,]  -3.8386514 170.992962  71.4164806\n[3,]  -0.1295953  71.416481 176.1875449\n```\n\n\n:::\n\n```{.r .cell-code}\nL %*% t(L)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]       [,2]        [,3]\n[1,] 200.0000000  -3.838651  -0.1295953\n[2,]  -3.8386514 170.992962  71.4164806\n[3,]  -0.1295953  71.416481 176.1875449\n```\n\n\n:::\n:::\n\n\nSome of you might note that all this fuss is irrelevant, as we essentially already had the Cholesky decomposition of $X^TX$.\nRemember that $X^TX = R^TQ^TQR^T = R^TR$, so if we set $L = R^T$ we are fairly close to the solution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nL\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]      [,2]     [,3]\n[1,] 14.142135624  0.000000  0.00000\n[2,] -0.271433644 13.073610  0.00000\n[3,] -0.009163773  5.462454 12.09748\n```\n\n\n:::\n\n```{.r .cell-code}\nt(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              [,1]       [,2]     [,3]\n[1,] -14.142135624   0.000000  0.00000\n[2,]   0.271433644 -13.073610  0.00000\n[3,]   0.009163773  -5.462454 12.09748\n```\n\n\n:::\n:::\n\n\nRemember that $R$ was unique only up to multiplication with a diagonal matrix consisting of ones and minus ones.\nIf we impose the restriction that $R$ only has positive diagonal elements, we get the following.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt(R) %*% diag(sign(diag(R)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]      [,2]     [,3]\n[1,] 14.142135624  0.000000  0.00000\n[2,] -0.271433644 13.073610  0.00000\n[3,] -0.009163773  5.462454 12.09748\n```\n\n\n:::\n\n```{.r .cell-code}\nL\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             [,1]      [,2]     [,3]\n[1,] 14.142135624  0.000000  0.00000\n[2,] -0.271433644 13.073610  0.00000\n[3,] -0.009163773  5.462454 12.09748\n```\n\n\n:::\n:::\n\n\nWhich illustrates that we already had this solution.\nHowever, computing the $QR$ decomposition first requires more calculations than necessary (i.e., is less efficient).\n:::\n\n## A note on computational complexity\n\nSo far, we have considered four methods for performing the calculations: solving the normal equations directly, using a Cholesky decomposition, and using a $QR$ or $SVD$ decomposition of the design matrix, and exploiting this structure to perform the calculations.\nEach of these methods requires a certain number of operations, leaving storage requirements aside.\nIn it's least optimized form, solving the normal equations directly requires approximately $2np^2 + 2np + p^3$ operations: $2np^2$ for computing $X^TX$, $2np$ for computing $X^Ty$ and $p^3$ for computing the inverse of $X^TX$.\nHowever, we can improve efficiency by noting that for calculating $X^TX$, we only need to perform half of the computations, because $X^TX$ yields a symmetric matrix.\nMoreover, performing a Cholesky decomposition to solve the system yields an additional speed-up.\nComputing the Cholesky decomposition of $X^TX$ requires $p^3/3$ operations, and subsequently solving the system requires another $2p^2$.\nSolving the normal equations by performing a $QR$ decomposition first requires approximately $2(n-p/3)p^2$ computations for performing the QR decomposition, $2np$ computations for computing $Q^T y$ and $p^2$ computations for solving the system.\nComputing the coefficients using a singular value decomposition does not yield an exact number of operations, but takes approximately $2(n + 11/2 p)p^2$ operations for computing the $SVD$, and then some additional operations for performing the matrix multiplications ($2np$ for calculating $U^Ty$, $p$ for multiplying with $\\Sigma^{-1}$ and $2p^2$ for multiplying this vector with $V$, if all done efficiently).\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- P+1\nn <- N\n2*n*p^2 + 2*n*p + p^3 # directly\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4827\n```\n\n\n:::\n\n```{.r .cell-code}\nn*p^2 + 2*n*p + p^3/3 + 2*p # optimized and with cholesky\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3015\n```\n\n\n:::\n\n```{.r .cell-code}\n2*(n - p/3)*p^2 + 2*n*p + p^2 # QR\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4791\n```\n\n\n:::\n\n```{.r .cell-code}\n2*(n + 11/2 * p)*p^2 + 3*p^2 + 2*n*p + p # SVD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5127\n```\n\n\n:::\n:::\n\n\n## Benchmarking\n\nThroughout, we displayed multiple ways of calculating the regression coefficients in `R`, and made some statements with respect to their efficiency.\nHere we very briefly benchmark the different ways of calculating the coefficients, to back up our earlier claims.\nIn this benchmark, we also include the fastest in-built `R` function to perform linear regression (`.lm.fit()`) as a comparison.\n\n### Small data benchmark\n\n\n::: {#tbl-benchmark .cell tbl-cap='Benchmarking different ways of calculating OLS regression coefficients with the example data.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> expression </th>\n   <th style=\"text-align:right;\"> min </th>\n   <th style=\"text-align:right;\"> median </th>\n   <th style=\"text-align:right;\"> itr/sec </th>\n   <th style=\"text-align:right;\"> mem_alloc </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> lm() </td>\n   <td style=\"text-align:right;\"> 250.4µs </td>\n   <td style=\"text-align:right;\"> 310.5µs </td>\n   <td style=\"text-align:right;\"> 3044.244 </td>\n   <td style=\"text-align:right;\"> 48.9KB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> .lm.fit() </td>\n   <td style=\"text-align:right;\"> 4.7µs </td>\n   <td style=\"text-align:right;\"> 6.9µs </td>\n   <td style=\"text-align:right;\"> 137200.767 </td>\n   <td style=\"text-align:right;\"> 10.8KB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> solve(t(X)%*%X)%*%t(X)%*%y </td>\n   <td style=\"text-align:right;\"> 17.1µs </td>\n   <td style=\"text-align:right;\"> 22µs </td>\n   <td style=\"text-align:right;\"> 44016.581 </td>\n   <td style=\"text-align:right;\"> 14.2KB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> solve(crossprod(X), crossprod(X, y)) </td>\n   <td style=\"text-align:right;\"> 6.8µs </td>\n   <td style=\"text-align:right;\"> 8.1µs </td>\n   <td style=\"text-align:right;\"> 114003.875 </td>\n   <td style=\"text-align:right;\"> 0B </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> qr.solve(X, y) </td>\n   <td style=\"text-align:right;\"> 22.4µs </td>\n   <td style=\"text-align:right;\"> 29.3µs </td>\n   <td style=\"text-align:right;\"> 32504.222 </td>\n   <td style=\"text-align:right;\"> 22.2KB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> svd </td>\n   <td style=\"text-align:right;\"> 19µs </td>\n   <td style=\"text-align:right;\"> 27.2µs </td>\n   <td style=\"text-align:right;\"> 36001.372 </td>\n   <td style=\"text-align:right;\"> 20.7KB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> chol </td>\n   <td style=\"text-align:right;\"> 12.4µs </td>\n   <td style=\"text-align:right;\"> 15µs </td>\n   <td style=\"text-align:right;\"> 63417.286 </td>\n   <td style=\"text-align:right;\"> 22.2KB </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSo, `.lm.fit()` is super fast, followed by `solve(crossprod(X), crossprod(X, y))`. Using the in-built `chol` function with forward and backward solving is also relatively fast, and even our inefficient `solve(t(X) %*% X) %*% t(X) %*% y` is not too slow. \n\n### Larger data benchmark\n\nWhen we increase the size of the data, we see what we expect to see.\n\n\n::: {#tbl-benchmark-large .cell tbl-cap='Benchmarking different ways of calculating OLS regression coefficients with a somewhat larger dataset.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> expression </th>\n   <th style=\"text-align:right;\"> min </th>\n   <th style=\"text-align:right;\"> median </th>\n   <th style=\"text-align:right;\"> itr/sec </th>\n   <th style=\"text-align:right;\"> mem_alloc </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> lm() </td>\n   <td style=\"text-align:right;\"> 22.6ms </td>\n   <td style=\"text-align:right;\"> 24.1ms </td>\n   <td style=\"text-align:right;\"> 41.38545 </td>\n   <td style=\"text-align:right;\"> 23.58MB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> .lm.fit() </td>\n   <td style=\"text-align:right;\"> 17.1ms </td>\n   <td style=\"text-align:right;\"> 18.6ms </td>\n   <td style=\"text-align:right;\"> 53.07588 </td>\n   <td style=\"text-align:right;\"> 3.93MB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> solve(t(X)%*%X)%*%t(X)%*%y </td>\n   <td style=\"text-align:right;\"> 39.3ms </td>\n   <td style=\"text-align:right;\"> 41.4ms </td>\n   <td style=\"text-align:right;\"> 23.52300 </td>\n   <td style=\"text-align:right;\"> 11.88MB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> solve(crossprod(X), crossprod(X, y)) </td>\n   <td style=\"text-align:right;\"> 11.8ms </td>\n   <td style=\"text-align:right;\"> 12.3ms </td>\n   <td style=\"text-align:right;\"> 78.91230 </td>\n   <td style=\"text-align:right;\"> 164.82KB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> qr.solve(X, y) </td>\n   <td style=\"text-align:right;\"> 18.9ms </td>\n   <td style=\"text-align:right;\"> 21ms </td>\n   <td style=\"text-align:right;\"> 47.40981 </td>\n   <td style=\"text-align:right;\"> 15.5MB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> svd </td>\n   <td style=\"text-align:right;\"> 63.6ms </td>\n   <td style=\"text-align:right;\"> 65.9ms </td>\n   <td style=\"text-align:right;\"> 14.97805 </td>\n   <td style=\"text-align:right;\"> 15.89MB </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> chol </td>\n   <td style=\"text-align:right;\"> 12.1ms </td>\n   <td style=\"text-align:right;\"> 13.6ms </td>\n   <td style=\"text-align:right;\"> 71.12533 </td>\n   <td style=\"text-align:right;\"> 241.73KB </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe function `solve(crossprod(X), crossprod(X, y))` is super fast because it uses a very efficient estimation routine that is highly optimized and performs barely any additional computations.\nThe Cholesky decomposition `chol()` with forward and backward substitution is also really efficient.\nThe functions `.lm.fit()` and `qr.solve()` are also rather fast.\nThe singular value decomposition is slower than the other functions, but has some advantages not shared by the other functions.\n\n## Reuse\n\n[GNU GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html)\n\n### Citation\n\n```{{bib}}\n@online{\n  author = {Volker, Thom Benjamin},\n  title = {Different ways of calculating OLS regression coefficients (in R)},\n  date = {2025-06-09},\n  url = {https://thomvolker.github.io/blog/2506_regression/}\n}\n```\n\nFor attribution, please cite this blogpost as:\n\n> Volker, T. B.\n> (2025).\n> Different ways of calculating OLS regression coefficients (in R).\n> Achieved from <https://thomvolker.github.io/blog/2506_regression/>\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}