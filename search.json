[
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "I fully support open educational materials, and I intend to make all teaching resources I developed available online. All materials below are licensed liberally. If the GitHub link is not there, there is probably exam material included in the GitHub repo, which prevents me from sharing it publicly. If you are interested in any of the source files, please drop me a line.\n\nWorkshops\n\n\n\n\n\n\n\n2023\nFake it ’till you make it: Generating synthetic data with high utility in R  Half-day course on synthetic data; developed with Erik-Jan van Kesteren ( GitHub)\n\n\n\n\n\nUniversity courses\n\n\n\n\n\n\n\n2022-2025\nBattling the curse of dimensionality (MSc. Course)\n\n\n2021-2026\nData wrangling and data analysis (MSc. Course)\n\n\n2024\nMachine Learning with Python (Summer School Course)\n\n\n2019-2022\nMultiple Imputation in Practice (Summer School Course)"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "densityratio\n\n\nAn R-package for distribution comparison through density ratio estimation. The package is actively being developed and suggestions are very welcome!\n\n\n\n\n\n\n\n\n\n\n\n\nmice\n\n\nI am a contributor to the mice package for multiple imputation. Among other things, we are currently developing a mice implementation for the software JASP.\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blogposts",
    "section": "",
    "text": "Below you find some of my writings on a variety of topics, ranging from data privacy to Bayesian methodology. Feel free to reach out if you have any questions or comments!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifferent ways of calculating OLS regression coefficients (in R)\n\n\n\nregression\n\nqr decomposition\n\nsingular value decomposition\n\ngradient descent\n\nmachine learning\n\n\n\nMany different ways of calculating OLS regression coefficients exist, but some ways are more efficient than others. In this post we discuss some of…\n\n\n\n\n\nJun 16, 2025\n\n\nThom Benjamin Volker\n\n\n\n\n\n\n\n\n\n\n\n\nHow does glmnet perform ridge regression?\n\n\n\nmachine learning\n\nregression\n\n\n\n\n\n\n\n\n\nMay 23, 2025\n\n\nThom Benjamin Volker\n\n\n\n\n\n\n\n\n\n\n\n\nNew website\n\n\n\nother\n\npersonal\n\n\n\n\n\n\n\n\n\nMay 16, 2025\n\n\nThom Benjamin Volker\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2505_new_website/index.html",
    "href": "blog/2505_new_website/index.html",
    "title": "New website",
    "section": "",
    "text": "Since today, I have a new personal website! The site mainly serves as a repository to host some of my research projects and developed course materials. I may also write an occasional blogpost!\nDeveloping the website was really a breeze, thanks to the quarto software and the materials by Samantha Shanny-Csik."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Thom Benjamin Volker",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n2022\nMSc. Methodology and Statistics cum laude\n\n\n2022\nMSc. Sociology and Social Research cum laude\n\n\n2019\nLiberal Arts & Sciences"
  },
  {
    "objectID": "about.html#selected-publications",
    "href": "about.html#selected-publications",
    "title": "Thom Benjamin Volker",
    "section": "Selected publications",
    "text": "Selected publications\n\nFor a full overview, please check my Google Scholar profile.\n\n\nKlugkist, I., & Volker, T. B. (2023). Bayesian evidence synthesis for informative hypotheses: An introduction. Psychological Methods. Advance online publication. (link)\n\n\nVolker, T. B., de Wolf, P. P., & van Kesteren, E. J. (2024). A density ratio framework for evaluating the utility of synthetic data. arXiv:2408.13167. (link)\n\n\nVolker, T. B., & Vink, G. (2021). Anonymiced shareable data: Using mice to create and analyze multiply imputed synthetic datasets. Psych, 3(4), 703-716. (link)"
  },
  {
    "objectID": "blog/2505_glmnet_ridge/index.html",
    "href": "blog/2505_glmnet_ridge/index.html",
    "title": "How does glmnet perform ridge regression?",
    "section": "",
    "text": "Today, I was doing some programming for the R software mice, and it turns out I needed to calculate the residual degrees of freedom after performing regression with a ridge penalty to improve numerical stability. Perhaps, I thought, the glmnet package might provide any help, and at least may serve to verify my understanding. Although I love the glmnet package, I should’ve known better. That is, the package is great for estimation, but makes some design choices that are buried somewhere deep in the documentation, which can make it hard to find what is actually going on under the hood. In the end, I found out how glmnet performs ridge regression, so all good, I suppose."
  },
  {
    "objectID": "blog/2505_glmnet_ridge/index.html#footnotes",
    "href": "blog/2505_glmnet_ridge/index.html#footnotes",
    "title": "How does glmnet perform ridge regression?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy default, glmnet fits a lasso regression model, which is fairly similar but uses an \\(\\ell_1\\) penalty for the coefficients, which results in some coefficients estimated to be precisely zero. Changing the alpha parameter yields a mixture between 1-alpha times the ridge penalty and alpha times the lasso penalty.↩︎"
  },
  {
    "objectID": "blog/2506_regression/index.html",
    "href": "blog/2506_regression/index.html",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "",
    "text": "Linear regression is life. At age 17, Carl Friedrich Gauss, a German polymath, thought to himself: “You know what, we need to be able to determine the best linear fit through this cloud of points”, and the rest is history. The goal is to predict a target variable, \\(y_i\\), for observation \\(i\\) from a set of predictors, \\(x_{i,1}, x_{i,2}, \\ldots, x_{i,p}\\). The model is given by \\[\ny_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\ldots + \\beta_p x_{i,p} + \\varepsilon_i,\n\\] where \\(\\beta_0\\) denotes the intercept, \\(\\beta_1, \\ldots, \\beta_p\\) are the regression coefficients corresponding to predictors \\(x_{i,1}, \\dots, x_{i,p}\\), and \\(\\varepsilon_i\\) is the error term. The intercept \\(\\beta_0\\) can be interpreted as the expected value of \\(y\\) for an observation that scores zero on all predictor variables, and \\(\\beta_1, \\ldots, \\beta_p\\) represent the change in the expected value of \\(y\\) for a one-unit increase in the corresponding predictor variable, holding all other predictors constant. It is often convenient to cast the model in matrix form: \\[\ny = X \\beta + \\varepsilon,\n\\] where \\(y\\) is the \\(n \\times 1\\) response vector, \\(X\\) is the \\(n \\times (p + 1)\\) design matrix containing the predictors, \\(\\beta\\) is the \\((p + 1) \\times 1\\) vector of coefficients, and \\(\\varepsilon\\) is the \\(n \\times 1\\) vector of errors. Note that the design matrix \\(X\\) contains a column of ones for the intercept, which we denote throughout as the “zeroth” column so that it corresponds to \\(\\beta_0\\), notation-wise. Hence, for each observation, we multiply each predictor score by the corresponding regression coefficient and sum the terms.\nThe goal, of course, is to estimate the regression coefficients \\(\\beta\\) such that we can predict the response variable \\(y\\) as good as it gets. You might want to do this by minimising the sum of squared errors: \\[\n\\min_\\beta \\sum_{i=1}^n (y_i - \\beta_0 - \\beta_1 x_{i,1} - \\ldots - \\beta_p x_{i,p})^2 = (y - X \\beta)^T (y - X \\beta).\n\\] The “best” estimate for this problem yields the smallest residuals (in a squared error sense). So, how do we choose the coefficients such that the residuals are as small as possible? One way would be to start with an initial guess for the regression coefficients and iteratively move in the direction where the errors decrease as fast as possible. If you look at the contour plot below, the residual sum of squares for some data \\(X\\) and \\(y\\) is displayed as a function of the two regression coefficients. At every location in the contour plot, the sum of squared errors slopes down in the direction of the minimum. So, if we knew how the sum of squares changes as a function of the regression coefficients, we could iteratively move our estimates of the regression coefficients in the direction where the residual sum of squares becomes smaller and smaller, until we end up at the minimum!\nShow figure code\nlibrary(ggplot2)\nlibrary(dplyr)\n\nbgrid &lt;- expand.grid(\n  b1 = seq(-0.3, 0.7, length.out = 100),\n  b2 = seq(0, 1, length.out = 100)\n)\n\nopt &lt;- coef(lm(y~Xd-1))\n\nrss &lt;- function(b, x, y, b0) {\n  yhat &lt;- b0 + X %*% b\n  sum((y - yhat)^2)\n}\n\nrss &lt;- mutate(bgrid, rss = apply(bgrid, 1, rss, x = X, y = y, b0 = opt[1]))\n\nggplot(rss, aes(b1, b2, z = rss)) +\n  geom_contour_filled() +\n  geom_point(aes(x = opt[2], y = opt[3]), color = \"red\", size = 3) +\n  labs(\n    title = \"Residual sum of squares\",\n    subtitle = \"Contours of residual sum of squares as a \nfunction of the regression coefficients\",\n    x = \"Coefficient for predictor 1\",\n    y = \"Coefficient for predictor 2\"\n  ) +\n  theme_minimal() +\n  theme(plot.background = element_rect(fill = \"#fffbf2\", colour = \"transparent\"))\n\n\n\n\n\n\n\n\nFigure 1\nTo know how the residual sum of squares changes as a function of the regression coefficient, we can make use of the derivatives of this function with respect to \\(\\beta\\). Loosely speaking, the derivatives quantify how fast the sum of squares changes when we change each of the coefficients by a tiny amount. The collection of these rates of change, a vector called the gradient, tells us in what direction we need to move to get closer to the minimum. The gradient is commonly denoted by \\(\\nabla_\\beta\\) (the \\(\\nabla\\) symbol is called nabla or del) and can be obtained by applying the chain rule for differentiation: \\[\n\\begin{aligned}\n\\nabla_\\beta &= \\begin{bmatrix}\n\\frac{\\partial}{\\partial \\beta_0}\n\\left(\\sum_{i=1}^n (y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots - \\beta_px_{i,p})^2 \\right)\\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_p} \\left(\\sum_{i=1}^n (y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots - \\beta_px_{i,p})^2 \\right)\n\\end{bmatrix} \\\\\n&= \\begin{bmatrix}\n\\left( \\sum_{i=1}^n -2 x_{i,0}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\vdots \\\\\n\\left( \\sum_{i=1}^n -2 x_{i,p}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\end{bmatrix}.\n\\end{aligned}\n\\] Each partial derivative tells us how the total error would change if we nudged one coefficient while keeping the others fixed. We can update each regression coefficient repeatedly by a tiny amount in the direction of the minimum, until we ultimately reach this minimum. This process is known as gradient descent. A short and applied illustration of gradient descent is included below, but since I don’t want to focus this post on gradient descent too much, I hid the algorithm behind an expandable block.\nWe can also use the gradient directly, by noting that in the optimum, the derivatives are zero, because moving further in the direction of the minimum is not possible, and thus there is “zero” change in this direction. Setting each derivative equal to zero and solving the equations for the regression coefficients yields the optimal solution in one go. This is something that can be done with linear algebra!"
  },
  {
    "objectID": "blog/2506_regression/index.html#from-derivatives-to-estimates",
    "href": "blog/2506_regression/index.html#from-derivatives-to-estimates",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "From derivatives to estimates",
    "text": "From derivatives to estimates\nIn the previous section, we saw that the gradient is given by \\[\n\\nabla_\\beta = \\begin{bmatrix}\n\\left( \\sum_{i=1}^n -2 x_{i,0}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\vdots \\\\\n\\left( \\sum_{i=1}^n -2 x_{i,p}(y_i - \\beta_0 x_{i,0} - \\beta_1 x_{i,1} - \\dots -\\beta_p x_{i,p}) \\right) \\\\\n\\end{bmatrix}.\n\\] In each row of this vector, the product term consistently multiplies the columns of our predictor matrix, \\(X\\), with the residuals \\(y - X\\beta\\). The same operations can be encoded in terms of matrix, \\[\n\\nabla_\\beta = -2 X^T(y - X\\beta),\n\\] which multiplies each column of \\(X\\) with the vector of residuals and sums the elements. We can get rid of the parentheses, such that we obtain \\[\n\\begin{aligned}\n\\nabla_\\beta &= -2 X^T(y - X\\beta) \\\\\n&= -2X^Ty + 2X^TX\\beta\n\\end{aligned}\n\\] Setting the derivative equal to zero and dividing both terms by \\(2\\) yields the set of equations \\[\nX^T X \\beta = X^T y,\n\\] which are commonly called the normal equations. We can solve these equations for \\(\\beta\\) by pre-multiplying both sides with the inverse of \\(X^T X\\), and noting that \\((X^T X)^{-1}(X^T X) = I\\), the identity matrix. We then obtain \\[\n\\begin{aligned}\n(X^T X)^{-1}(X^T X)\\beta &= (X^T X)^{-1} X^T y \\\\\n\\beta &= (X^T X)^{-1} X^T y.\n\\end{aligned}\n\\] Hooray! With just a few steps, we get from the gradient to the solution of the regression problem."
  },
  {
    "objectID": "blog/2506_regression/index.html#solving-the-normal-equations-in-r",
    "href": "blog/2506_regression/index.html#solving-the-normal-equations-in-r",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "Solving the normal equations in R",
    "text": "Solving the normal equations in R\nCoding the solution to our regression problem takes just a couple of operations. Recall that our design matrix Xd includes a column of ones for the intercept. The solution is given by\n\nsolve(t(Xd) %*% Xd) %*% t(Xd) %*% y\n\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n\n\nWe can verify whether our obtained solution equals the solution provided by the regression function in R by calling lm().\n\nlm_fit &lt;- lm(y ~ X)\ncoef(lm_fit)\n\n(Intercept)          X1          X2 \n-0.05924251  0.12069667  0.52018686 \n\n\nNot very surprisingly, the coefficients are equal. However, I can directly disclose that our solution is correct, but can be improved in various ways. First, inverting a matrix is expensive, and calculating the cross products as t(Xd) %*% Xd and t(Xd) %*% y is also not very efficient. We can already achieve a speed-up by using the cross-product function crossprod(), which is typically more efficient than separately transposing and multiplying matrices. Moreover, we do not need to invert the matrix \\(X^T X\\), but we can solve the normal equations directly, which also yields a speed-up (Table 1 provides some evidence for these claims). In terms of code, this is achieved as follows, which again yields the same coefficients.\n\nsolve(crossprod(Xd), crossprod(Xd, y))\n\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n\n\nThis is almost as efficient as it gets, computationally, because internally, R uses some clever tricks to solve this system of equations as efficiently as possible. However, there are other ways to obtaining the regression coefficients that have different advantages. In the next sections, we look into some of the most popular linear algebra tricks to compute the regression coefficients."
  },
  {
    "objectID": "blog/2506_regression/index.html#solving-the-normal-equations-through-a-qr-decomposition",
    "href": "blog/2506_regression/index.html#solving-the-normal-equations-through-a-qr-decomposition",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "Solving the normal equations through a QR decomposition",
    "text": "Solving the normal equations through a QR decomposition\nAn alternative way to solve the normal equations is by first performing a transformation of the design matrix \\(X\\) that simplifies the subsequent calculations. To this end, we can decompose the design matrix \\(X\\) into an \\(n \\times p+1\\) orthogonal unitary matrix \\(Q\\) and an upper triangular matrix \\(R\\) using a QR decomposition. This sounds really complicated, but it merely means that the matrix \\(Q\\) contains \\(p+1\\) uncorrelated (i.e., orthogonal) columns of length one (i.e., the sum of the squared elements of every column in \\(Q\\) equals \\(\\sum_{i=1}^n Q_{(i,j)}^2 = ||Q_{.,j}||_2 = Q_{.,j}^TQ_{.,j} = 1\\)). The matrix \\(R\\) is then chosen such that when multiplied with \\(Q\\), we obtain \\(X\\) again. For those unfamiliar with QR decompositions, a simple algorithm for computing the QR decomposition is included below.\n\n\n\n\n\n\nA simple algorithm for computing the QR decomposition (Gram-Schmidt)\n\n\n\n\n\nNote that the algorithm below is included solely for explanatory purposes for those unaware of the QR decomposition. It is poor code from a computational point of view.\n\nqr_func &lt;- function(X) {\n  # store dimensions of input matrix\n  dim &lt;- dim(X)\n  # initialize empty Q matrix\n  Q &lt;- matrix(0, dim[1], dim[2])\n  # note the convenient feature of Q that the zero columns don't do anything\n  # initialize the first column of Q as a scaled version of the first column of X\n  Q[,1] &lt;- X[,1] / c(sqrt(crossprod(X[,1])))\n  for (j in 2:dim[2]) {\n    # calculate coefficients that produce X[,j] orthogonal to existing Q columns\n    b &lt;- c(crossprod(X[,j], Q[,1:(j-1)]) / apply(Q[,1:(j-1), drop = FALSE], 2, crossprod))\n    # note that these coefficients are merely the unscaled covariances of X[,j] \n    # with the existing columns of Q, and by multiplying existing Q with these \n    # coefficients and subtracting these values from X, the new Q-column is \n    # uncorrelated to the existing Q-columns. \n    Q[,j] &lt;- X[,j] - Q[,1:(j-1), drop = FALSE] %*% b\n    # scale to unit length\n    Q[,j] &lt;- Q[,j] / sqrt(sum(Q[,j]^2))\n  }\n  # calculate R by making sure that Q^T X = R, and thus X = QR\n  # we use here that Q is orthonormal, and thus Q^TQ = I\n  R &lt;- t(Q) %*% X\n\n  list(Q = Q, R = R)\n}\n\nK &lt;- matrix(sample(20), 5)\n\n(qr_own &lt;- qr_func(K))\n\n$Q\n           [,1]       [,2]        [,3]       [,4]\n[1,] 0.06696495  0.4354260 -0.11828785  0.3418044\n[2,] 0.20089486  0.7514209 -0.10300605  0.2878141\n[3,] 0.66964953  0.1928315  0.01205777 -0.7037544\n[4,] 0.53571962 -0.2341348  0.67174297  0.4547543\n[5,] 0.46875467 -0.3921322 -0.72388793  0.3134662\n\n$R\n              [,1]          [,2]         [,3]      [,4]\n[1,]  2.986637e+01  1.720999e+01 2.604937e+01 20.960030\n[2,]  1.776357e-15  1.802266e+01 7.528889e+00  8.005370\n[3,] -7.105427e-15 -5.995204e-15 8.645595e+00  1.010103\n[4,] -4.440892e-15 -9.547918e-15 2.442491e-15  6.524636\n\n\nNote that the matrix \\(R\\) is unique only up to multiplication by a diagonal matrix consisting of positive and negative ones. That is, if we multiply a column of \\(Q\\) by \\(-1\\) and perform the same operation to the row of \\(R\\), we obtain the same \\(X\\) upon multiplication.\n\nqr_R &lt;- qr(K)\nQ &lt;- qr.Q(qr_R)\nR &lt;- qr.R(qr_R)\nsign_R &lt;- sign(diag(R))\n\nall.equal(Q, qr_own$Q %*% diag(sign_R))\n\n[1] TRUE\n\nall.equal(R, diag(sign_R) %*% qr_own$R)\n\n[1] TRUE\n\n\n\n\n\nUsing some convenient features of orthonormal and upper triangular matrices, it is actually quite easy to perform linear regression right now. Using the fact that \\(X = QR\\), we can rewrite our linear regression problem as \\[\n\\begin{aligned}\nX^TX \\beta &= X^Ty \\\\\n(QR)^T(QR) \\beta &= (QR)^T y \\\\\nR^TQ^TQR \\beta &= R^TQ^T y. \\\\\n\\end{aligned}\n\\] So far, it seems like we haven’t achieved much, other than rewriting some expression into a longer and not necessarily simpler expression. However, remember that \\(Q\\) is orthonormal, and thus that \\(Q^TQ = I\\). So, now we have \\[\nR^TR \\beta = R^TQ^Ty,\n\\] which is almost as simple as the expression we already had. We can again pre-multiply both sides with the inverse of \\(R^T\\), which results in two new identity matrices that we can ignore. We now obtain the final expression \\[\nR\\beta = Q^T y,\n\\] and since \\(R\\) is upper triangular, we can very efficiently solve this set of equations using back-substitution. In our case, we have only two predictors and an intercept, so \\(R\\) is \\(3 \\times 3\\) with zeros below the diagonal, \\(\\beta\\) is a vector with three elements, and \\(Q^Ty\\) is a vector with three elements. Hence, we have \\[\n\\begin{aligned}\nR_{1,1}\\beta_1 + R_{1,2}\\beta_2 + R_{1,3}\\beta_3 &= (Q^T y)_1 \\\\\nR_{2,2}\\beta_2 + R_{2,3}\\beta_3 &= (Q^T y)_2 \\\\\nR_{3,3}\\beta_3 &= (Q^Ty)_3.\n\\end{aligned}\n\\] Starting from the bottom, \\(\\beta_3\\) is given by \\((Q^Ty)_3 / R_{3,3}\\). The value for \\(\\beta_2\\) is easily obtained once we have \\(\\beta_3\\)! The only thing we have to do is rewriting the equation above as \\(\\beta_2 = ((Q^T y)_{2} - R_{2,3} \\beta_3)/R_{2,2}\\). Doing the same thing for the intercept yields \\(\\beta_1 = ((Q^T y)_{1} - R_{1,2}\\beta_2 -  R_{1,3} \\beta_3)/R_{1,1}\\). Yoo, this stuff is actually simpler than I thought!\n\nQR &lt;- qr(Xd)\nQTy &lt;- t(qr.Q(QR)) %*% y\nR &lt;- qr.R(QR)\n\nb3 &lt;- QTy[3] / R[3,3]\nb2 &lt;- (QTy[2] - R[2,3] * b3) / R[2,2]\nb1 &lt;- (QTy[1] - R[1,2] * b2 - R[1,3] * b3) / R[1,1]\n\nc(b1, b2, b3)\n\n[1] -0.05924251  0.12069667  0.52018686\n\n\n\n\n\n\n\n\nWe can do the same thing much more efficient, just watch!\n\n\n\n\nqr.solve(Xd, y)\n\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n\n\nInternally, solve(crossprod(X), crossprod(X, y)) uses a similar trick (by decomposing \\(X^TX = LU\\)), so we don’t necessarily expect a speed-up here.\n\n\nUsing a \\(QR\\) decomposition to solve the least-squares equations is computationally more stable (i.e., results in smaller approximation errors) than solving the normal equations directly. If \\(X^TX\\) is far from singular, this does not matter much, but if some variables are highly correlated, the resulting approximation errors can be quite large. However, the computation is somewhat less efficient than solving the normal equations directly. We will come back to computational efficiency below."
  },
  {
    "objectID": "blog/2506_regression/index.html#same-trick-different-singular-value-decomposition",
    "href": "blog/2506_regression/index.html#same-trick-different-singular-value-decomposition",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "Same trick, different (singular value) decomposition",
    "text": "Same trick, different (singular value) decomposition\nIf you get the gist of the previous method, the singular value decomposition will feel fairly easy. We again start from the setting that \\(X^TX\\beta = X^Ty\\), but now we replace \\(X\\) by its singular value decomposition \\(U\\Sigma V^T\\), where both \\(U\\) and \\(V\\) are orthonormal matrices and \\(\\Sigma\\) is a diagonal matrix containing the singular values of \\(X\\). We now obtain the following expression \\[\nV\\Sigma^T U^T U \\Sigma V^T \\beta = V \\Sigma^T U^T y.\n\\] We know from the previous section that the product of orthonormal matrices \\(U^T U\\) can be ignored, and since \\(\\Sigma\\) is diagonal, \\(\\Sigma^T = \\Sigma\\). So we can rewrite the expression as \\[\n\\begin{aligned}\nV\\Sigma^T U^T U \\Sigma V^T \\beta &= V \\Sigma^T U^T y \\\\\n\\Rightarrow V\\Sigma\\Sigma V^T \\beta &= V \\Sigma U^T y \\\\\n\\Rightarrow \\Sigma\\Sigma V^T \\beta &= \\Sigma U^T y\\\\\n\\Rightarrow V^T \\beta &= \\Sigma^{-1} U^Ty \\\\\n\\Rightarrow \\beta &= V \\Sigma^{-1} U^T y,\n\\end{aligned}\n\\] where we performed the following operations:\n\nOn line 2: remove \\(U^TU\\), as it’s just cluttered notation for an identity matrix.\nOn line 3: premultiply both sides by \\(V^T\\) and note that \\(V^TV\\) is also an identity matrix.\nOn line 4: premultiply both sides by \\(\\Sigma^{-1}\\Sigma^{-1}\\).\nOn line 5: premultiply both sides by \\(V\\) (and use orthogonality of \\(V\\)).\n\nIn R, we thus have the following:\n\nUSV &lt;- svd(Xd)\nUSV$v %*% diag(1/USV$d) %*% t(USV$u) %*% y\n\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n\n\nThese calculations can be further simplified because we do not need to construct the diagonal matrix \\(\\Sigma^{-1}\\). We can carry out these calculations element-wise on the \\(U^Ty\\) vector, like so:\n\nUSV$v %*% (crossprod(USV$u, y)/USV$d)\n\n            [,1]\n[1,] -0.05924251\n[2,]  0.12069667\n[3,]  0.52018686\n\n\nThe singular value decomposition is again slightly more expensive to compute than the \\(QR\\) decomposition, and is also more stable computationally than solving the normal equations directly. Moreover, the singular value decomposition can be used even if \\(X^TX\\) is rank-deficient, i.e., if some columns of \\(X\\) are linearly dependent. In such instances, we use \\[\n\\Sigma^+ = \\begin{cases}\n\\frac{1}{\\sigma_i} & \\text{if } \\sigma_i&gt;0 \\\\\n0 & \\text{otherwise}\n\\end{cases},\n\\] which is the pseudo-inverse of the matrix \\(\\Sigma\\) that can be used to calculate the pseudo-inverse of a rank-deficient matrix \\(X^TX\\). This formulation can even be used if we have more variables than cases, which is sometimes called minimum \\(\\ell_2\\) norm regression (see, for example, Tibshirani, 2024 and Kobak et al., 2020)."
  },
  {
    "objectID": "blog/2506_regression/index.html#one-final-decomposition-cholesky",
    "href": "blog/2506_regression/index.html#one-final-decomposition-cholesky",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "One final decomposition (Cholesky)",
    "text": "One final decomposition (Cholesky)\nWe circle back to our initial solution \\[\nX^TX\\beta = X^Ty,\n\\] and introduce a final decomposition to solve the set of equations. By noting that \\(X^TX\\) is symmetric and positive-definite, we can decompose it as \\[\nX^TX = LL^T,\n\\] where \\(L\\) is a lower-triangular matrix. So, instead of decomposing the design matrix (i.e., the matrix with observed predictors), we decompose the (unscaled) covariance matrix \\(X^TX\\). Using this formulation, we can write \\[\nLL^T\\beta = X^Ty.\n\\] Now, because \\(L\\) is lower-triangular, and \\(L^T\\) is upper-triangular, we can solve the set of equations efficiently as follows. First, we can solve \\[\nL g = X^Ty\n\\] for a newly introduced vector \\(g\\), which is easy because \\(L\\) is lower-triangular. To do this, we use forward substitution, which implies nothing more than the following. Starting from the first row, we have \\(L_{11}g_1 = (X^Ty)_1\\). For the second row, we have \\(L_{21} g_1 + L_{22}g_2 = (X^Ty)_2\\) for which we can plug in the previously calculated \\(g_1\\). We repeat the process until we solved for all elements of \\(g\\). Subsequently, we can solve \\(L^T\\beta = g\\), which is also easy because we can make use of the fact that \\(L^T\\) is upper triangular, which means we can use backward substitution. Here, starting from the last row, we have \\((L^T)_{pp}\\beta_p = g_p\\), \\((L^T)_{p-1}\\beta_{p-1} = g_{p-1}\\), and so on. To see why this works, we only have to rewrite the equation \\[\n\\begin{aligned}\nLL^T\\beta &= X^Ty \\\\\n\\Rightarrow ~~~~ LL^T\\beta &= Lg ~~~~~~ \\text{rewrite } X^Ty \\text{ as } Lg \\\\\n\\Rightarrow ~~~~~~~ L^T\\beta &= g ~~~~~~~~ \\text{premultiply with } L^{-1}.\n\\end{aligned}\n\\] This is one of the most efficient ways to calculate the regression coefficients, but can be numerically unstable when \\(X\\) is close to rank-deficient.\n\n\n\n\n\n\nComputing the Cholesky decomposition\n\n\n\n\n\nTo compute the Cholesky decomposition, we need to find a lower-triangular matrix \\(L\\) such that, when multiplied with its transpose, it produces the original matrix \\(X^TX\\). Let’s break this down into a set of equations, where we write \\(X^TX = A = LL^T\\): \\[\n\\begin{aligned}\nA &= LL^T \\\\\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1p} \\\\\na_{21} & a_{22} & \\cdots & a_{2p} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{p1} & a_{p2} & \\cdots & a_{pp}\n\\end{bmatrix} &= \\begin{bmatrix}\nl_{11} & 0 & \\cdots & 0 \\\\\nl_{21} & l_{22} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nl_{p1} & l_{p2} & \\cdots & l_{pp}\n\\end{bmatrix} \\begin{bmatrix}\nl_{11} & l_{21} & \\cdots & l_{p1} \\\\\n0 & l_{22} & \\cdots & l_{p2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & l_{pp}\n\\end{bmatrix}.\n\\end{aligned}\n\\] From here, it follows that, for the first row of \\(A\\), we have \\[\n\\begin{aligned}\na_{11} &= l_{11} \\cdot l_{11} + 0 \\cdot 0 + \\cdots + 0 \\cdot 0 = l_{11}^2 \\\\\na_{12} &= l_{11} \\cdot l_{21} + 0 \\cdot l_{22} + \\cdots + 0 \\cdot 0 = l_{11} \\cdot l_{21} \\\\\n\\vdots ~~ &= ~~~~~~~~~~~~~~~~~~~~~~\\vdots \\\\\na_{1p} &= l_{11} \\cdot l_{p1} + 0 \\cdot l_{p2} + \\cdots + 0 \\cdot l_{pp} = l_{11} \\cdot l_{p1}.\n\\end{aligned}\n\\] Thus, \\(l_{11} = \\sqrt{a_{11}}\\), \\(l_{21} = a_{12}/l_{11}\\), using \\(l_{11}\\) obtained in the previous step, and \\(l_{p_1}=a_{1p}/l_{11}\\), again using the previously obtained \\(l_{11}\\). Moving to the second row of \\(A\\), we have that \\[\na_{21} = a_{12},\n\\] which we have obtained already. Continuing our journey, we get \\[\n\\begin{aligned}\na_{22} &= l_{21} \\cdot l_{21} + l_{22} \\cdot l_{22} + 0 + \\cdots + 0 = l_{21}^2 + l_{22}^2 \\\\\n\\vdots ~~ &= ~~~~~~~~~~~~~~~~~~~~~~\\vdots \\\\\na_{2p} &= l_{21} \\cdot l_{p1} + l_{22} \\cdot l_{p2} + 0 + \\cdots + 0 = l_{21} \\cdot l_{p1} + l_{22} \\cdot l_{p2},\n\\end{aligned}\n\\] which we can again solve sequentially, \\[\n\\begin{aligned}\nl_{22} &= \\sqrt{a_{22} - l_{21}} \\\\\n\\vdots &= ~~~~~~~~~~ \\vdots \\\\\nl_{p2} &= \\left( a_{2p} - l_{21} \\cdot l_{p1} \\right)/l_{22},\n\\end{aligned}\n\\] and noting that every required factor of \\(L\\) has been calculated previously.\nPutting everything in a small algorithm, we obtain the following Cholesky-Crout algorithm (see Wikipedia:\n\ncholesky &lt;- function(XTX) {\n  dim &lt;- dim(XTX)\n  L &lt;- matrix(0, dim[1], dim[2])\n  L[1,1] &lt;- sqrt(XTX[1,1])\n  for (i in 2:dim[1]) {\n    for (j in 1:(i-1)) {\n      L[i,j] &lt;- (XTX[i,j] - sum(L[i,1:j]*L[j,1:j]))/L[j,j] # \n    }\n    L[i,i] &lt;- sqrt(XTX[i,i] - sum(L[i,1:(i-1)]^2))\n  }\n  L\n}\n\nXTX &lt;- crossprod(Xd)\nL &lt;- cholesky(XTX)\n\nXTX\n\n            [,1]       [,2]        [,3]\n[1,] 200.0000000  -3.838651  -0.1295953\n[2,]  -3.8386514 170.992962  71.4164806\n[3,]  -0.1295953  71.416481 176.1875449\n\nL %*% t(L)\n\n            [,1]       [,2]        [,3]\n[1,] 200.0000000  -3.838651  -0.1295953\n[2,]  -3.8386514 170.992962  71.4164806\n[3,]  -0.1295953  71.416481 176.1875449\n\n\nSome of you might note that all this fuss is irrelevant, as we essentially already had the Cholesky decomposition of \\(X^TX\\). Remember that \\(X^TX = R^TQ^TQR^T = R^TR\\), so if we set \\(L = R^T\\) we are fairly close to the solution.\n\nL\n\n             [,1]      [,2]     [,3]\n[1,] 14.142135624  0.000000  0.00000\n[2,] -0.271433644 13.073610  0.00000\n[3,] -0.009163773  5.462454 12.09748\n\nt(R)\n\n              [,1]       [,2]     [,3]\n[1,] -14.142135624   0.000000  0.00000\n[2,]   0.271433644 -13.073610  0.00000\n[3,]   0.009163773  -5.462454 12.09748\n\n\nRemember that \\(R\\) was unique only up to multiplication with a diagonal matrix consisting of ones and minus ones. If we impose the restriction that \\(R\\) only has positive diagonal elements, we get the following.\n\nt(R) %*% diag(sign(diag(R)))\n\n             [,1]      [,2]     [,3]\n[1,] 14.142135624  0.000000  0.00000\n[2,] -0.271433644 13.073610  0.00000\n[3,] -0.009163773  5.462454 12.09748\n\nL\n\n             [,1]      [,2]     [,3]\n[1,] 14.142135624  0.000000  0.00000\n[2,] -0.271433644 13.073610  0.00000\n[3,] -0.009163773  5.462454 12.09748\n\n\nWhich illustrates that we already had this solution. However, computing the \\(QR\\) decomposition first requires more calculations than necessary (i.e., is less efficient)."
  },
  {
    "objectID": "blog/2506_regression/index.html#a-note-on-computational-complexity",
    "href": "blog/2506_regression/index.html#a-note-on-computational-complexity",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "A note on computational complexity",
    "text": "A note on computational complexity\nSo far, we have considered four methods for performing the calculations: solving the normal equations directly, using a Cholesky decomposition, and using a \\(QR\\) or \\(SVD\\) decomposition of the design matrix, and exploiting this structure to perform the calculations. Each of these methods requires a certain number of operations, leaving storage requirements aside. In it’s least optimized form, solving the normal equations directly requires approximately \\(2np^2 + 2np + p^3\\) operations: \\(2np^2\\) for computing \\(X^TX\\), \\(2np\\) for computing \\(X^Ty\\) and \\(p^3\\) for computing the inverse of \\(X^TX\\). However, we can improve efficiency by noting that for calculating \\(X^TX\\), we only need to perform half of the computations, because \\(X^TX\\) yields a symmetric matrix. Moreover, performing a Cholesky decomposition to solve the system yields an additional speed-up. Computing the Cholesky decomposition of \\(X^TX\\) requires \\(p^3/3\\) operations, and subsequently solving the system requires another \\(2p^2\\). Solving the normal equations by performing a \\(QR\\) decomposition first requires approximately \\(2(n-p/3)p^2\\) computations for performing the QR decomposition, \\(2np\\) computations for computing \\(Q^T y\\) and \\(p^2\\) computations for solving the system. Computing the coefficients using a singular value decomposition does not yield an exact number of operations, but takes approximately \\(2(n + 11/2 p)p^2\\) operations for computing the \\(SVD\\), and then some additional operations for performing the matrix multiplications (\\(2np\\) for calculating \\(U^Ty\\), \\(p\\) for multiplying with \\(\\Sigma^{-1}\\) and \\(2p^2\\) for multiplying this vector with \\(V\\), if all done efficiently).\n\np &lt;- P+1\nn &lt;- N\n2*n*p^2 + 2*n*p + p^3 # directly\n\n[1] 4827\n\nn*p^2 + 2*n*p + p^3/3 + 2*p # optimized and with cholesky\n\n[1] 3015\n\n2*(n - p/3)*p^2 + 2*n*p + p^2 # QR\n\n[1] 4791\n\n2*(n + 11/2 * p)*p^2 + 3*p^2 + 2*n*p + p # SVD\n\n[1] 5127"
  },
  {
    "objectID": "blog/2506_regression/index.html#benchmarking",
    "href": "blog/2506_regression/index.html#benchmarking",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "Benchmarking",
    "text": "Benchmarking\nThroughout, we displayed multiple ways of calculating the regression coefficients in R, and made some statements with respect to their efficiency. Here we very briefly benchmark the different ways of calculating the coefficients, to back up our earlier claims. In this benchmark, we also include the fastest in-built R function to perform linear regression (.lm.fit()) as a comparison.\n\nSmall data benchmark\nWe first benchmark the different methods with the data used throughout. Hence, we have two variables and an additional intercept for \\(N = 200\\) observations.\n\n\nClick to the R-code used for benchmarking.\ncheck_func &lt;- function(target, current) {\n  all.equal(target, current, check.class = FALSE, check.attributes = FALSE)\n}\n\nbench::mark(\n  `lm()` = coef(lm(y ~ X)),\n  `.lm.fit()` = .lm.fit(Xd, y)$coefficients,\n  `solve(t(X)%*%X)%*%t(X)%*%y` = solve(t(Xd) %*% Xd) %*% t(Xd) %*% y,\n  `solve(crossprod(X), crossprod(X, y))` = solve(crossprod(Xd), crossprod(Xd, y)),\n  `qr.solve(X, y)` = qr.solve(Xd, y),\n  svd = {USV &lt;- svd(Xd); USV$v %*% (crossprod(USV$u, y)/USV$d)},\n  chol = {cholX &lt;- chol(crossprod(Xd)); backsolve(cholX, forwardsolve(t(cholX), crossprod(Xd, y)))},\n  check = check_func,\n  min_iterations = 100\n) |&gt; \n  dplyr::select(expression, min, median, `itr/sec`, `mem_alloc`) |&gt; \n  knitr::kable() |&gt;\n  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\nTable 1: Benchmarking different ways of calculating OLS regression coefficients with the example data.\n\n\n\n\n\n\nexpression\nmin\nmedian\nitr/sec\nmem_alloc\n\n\n\n\nlm()\n221.1µs\n254.7µs\n3736.127\n48.9KB\n\n\n.lm.fit()\n3.9µs\n5.2µs\n158021.679\n10.8KB\n\n\nsolve(t(X)%*%X)%*%t(X)%*%y\n14.6µs\n17.7µs\n52362.987\n14.2KB\n\n\nsolve(crossprod(X), crossprod(X, y))\n5.9µs\n6.5µs\n145107.343\n0B\n\n\nqr.solve(X, y)\n20.6µs\n26.6µs\n34978.790\n22.2KB\n\n\nsvd\n17.4µs\n22.5µs\n42917.865\n20.7KB\n\n\nchol\n11.5µs\n12.9µs\n75512.511\n22.2KB\n\n\n\n\n\n\n\n\nSo, .lm.fit() is super fast, followed by solve(crossprod(X), crossprod(X, y)). Using the in-built chol function with forward and backward solving is also relatively fast, and even our inefficient solve(t(X) %*% X) %*% t(X) %*% y is not too slow.\n\n\nLarger data benchmark\nWhen we increase the size of the data to \\(P = 100\\) variables and an additional intercept for \\(N = 5000\\) observations, we see what we expect to see.\n\n\nClick to see the R-code used for benchmarking.\nN &lt;- 5000\nP &lt;- 100\nX &lt;- mvtnorm::rmvnorm(\n  N,\n  rep(0, P),\n  diag(P)\n)\ny &lt;- rnorm(N)\nXd &lt;- cbind(1, X)\n\nbench::mark(\n  `lm()` = coef(lm(y ~ X)),\n  `.lm.fit()` = .lm.fit(Xd, y)$coefficients,\n  `solve(t(X)%*%X)%*%t(X)%*%y` = solve(t(Xd) %*% Xd) %*% t(Xd) %*% y,\n  `solve(crossprod(X), crossprod(X, y))` = solve(crossprod(Xd), crossprod(Xd, y)),\n  `qr.solve(X, y)` = qr.solve(Xd, y),\n  svd = {USV &lt;- svd(Xd); USV$v %*% (crossprod(USV$u, y)/USV$d)},\n  chol = {cholX &lt;- chol(crossprod(Xd)); backsolve(cholX, forwardsolve(t(cholX), crossprod(Xd, y)))},\n  check = check_func,\n  min_iterations = 100\n) |&gt; \n  dplyr::select(expression, min, median, `itr/sec`, `mem_alloc`) |&gt; \n  knitr::kable() |&gt;\n  kableExtra::kable_styling(full_width = FALSE, bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\nTable 2: Benchmarking different ways of calculating OLS regression coefficients with a somewhat larger dataset.\n\n\n\n\n\n\nexpression\nmin\nmedian\nitr/sec\nmem_alloc\n\n\n\n\nlm()\n20.3ms\n21.7ms\n44.16363\n23.58MB\n\n\n.lm.fit()\n15.3ms\n16.3ms\n61.09651\n3.93MB\n\n\nsolve(t(X)%*%X)%*%t(X)%*%y\n34.7ms\n38.4ms\n25.98245\n11.88MB\n\n\nsolve(crossprod(X), crossprod(X, y))\n10.2ms\n11.1ms\n88.58151\n164.82KB\n\n\nqr.solve(X, y)\n16.4ms\n20.4ms\n48.56595\n15.5MB\n\n\nsvd\n58ms\n62.5ms\n15.68589\n15.89MB\n\n\nchol\n10.2ms\n11.2ms\n89.12716\n241.73KB\n\n\n\n\n\n\n\n\nThe function solve(crossprod(X), crossprod(X, y)) is super fast because it uses a very efficient estimation routine that is highly optimized and performs barely any additional computations. The Cholesky decomposition chol() with forward and backward substitution is also really efficient. The functions .lm.fit() and qr.solve() are also rather fast. The singular value decomposition is slower than the other functions, but has some advantages not shared by the other functions."
  },
  {
    "objectID": "blog/2506_regression/index.html#reuse",
    "href": "blog/2506_regression/index.html#reuse",
    "title": "Different ways of calculating OLS regression coefficients (in R)",
    "section": "Reuse",
    "text": "Reuse\nGNU GPLv3\n\nCitation\n```{bib}\n@online{\n  author = {Volker, Thom Benjamin},\n  title = {Different ways of calculating OLS regression coefficients (in R)},\n  date = {2025-06-09},\n  url = {https://thomvolker.github.io/blog/2506_regression/}\n}\n```\nFor attribution, please cite this blogpost as:\n\nVolker, T. B. (2025). Different ways of calculating OLS regression coefficients (in R). Achieved from https://thomvolker.github.io/blog/2506_regression/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thom Volker",
    "section": "",
    "text": "I’m doing a PhD on synthetic data and privacy-preserving statistics, which is just a fancy way of saying I make fake data that looks real. Besides, I procrastinate by reading on Bayesian methods, marginally improving my R packages or giving workshops on any these topics.\nIf you want to collaborate, check out my GitHub for active projects. I work, among other things, on synthetic data, differential privacy, missing data imputation, Bayesian hypothesis testing (here and here) and density ratio estimation. If I don’t answer your email, I’m probably riding my bike. Sorry!"
  },
  {
    "objectID": "index.html#hi-im-thom",
    "href": "index.html#hi-im-thom",
    "title": "Thom Volker",
    "section": "",
    "text": "I’m doing a PhD on synthetic data and privacy-preserving statistics, which is just a fancy way of saying I make fake data that looks real. Besides, I procrastinate by reading on Bayesian methods, marginally improving my R packages or giving workshops on any these topics.\nIf you want to collaborate, check out my GitHub for active projects. I work, among other things, on synthetic data, differential privacy, missing data imputation, Bayesian hypothesis testing (here and here) and density ratio estimation. If I don’t answer your email, I’m probably riding my bike. Sorry!"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Below, you can find a list of talks I have given. The list is not exhaustive, but I will try to keep it up to date. I try to make all materials openly available. Please reach out if you have any questions or if you want to collaborate on any of the topics.\n\nInvited talks\n\n\n\n\n\n\n\n2024\nEvaluating the quality of synthetic data: A density ratio approach  At the Leiden Computational Network Science group\n\n\n2024\nMeasuring utility of synthetic data (in Dutch)  At the Dutch Expert Meeting on Synthetic Data\n\n\n\n\n\nConference presentations\n\n\n\n\n\n\n\n2023\nDensity ratios to evaluate and improve the utility of synthetic data  At the IOPS Conference 2023\n\n\n2023\nAssessing the utility of synthetic data: A density ratio perspective (paper)  At the United Nations Economic Commission for Europe Expert Meeting on Statistical Data Confidentiality"
  }
]